{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-IqJAMkwnCF"
   },
   "source": [
    "# Advanced Retrieval with LangChain\n",
    "\n",
    "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
    "\n",
    "We'll touch on:\n",
    "\n",
    "- Naive Retrieval\n",
    "- Best-Matching 25 (BM25)\n",
    "- Multi-Query Retrieval\n",
    "- Parent-Document Retrieval\n",
    "- Contextual Compression (a.k.a. Rerank)\n",
    "- Ensemble Retrieval\n",
    "- Semantic chunking\n",
    "\n",
    "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
    "\n",
    "There will be two breakout rooms:\n",
    "\n",
    "- 🤝 Breakout Room Part #1\n",
    "  - Task 1: Getting Dependencies!\n",
    "  - Task 2: Data Collection and Preparation\n",
    "  - Task 3: Setting Up QDrant!\n",
    "  - Task 4-10: Retrieval Strategies\n",
    "- 🤝 Breakout Room Part #2\n",
    "  - Activity: Evaluate with Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rKP3hgHivpe"
   },
   "source": [
    "# 🤝 Breakout Room Part #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xes8oT-xHN7"
   },
   "source": [
    "## Task 1: Getting Dependencies!\n",
    "\n",
    "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7OHJXzfyJyA"
   },
   "source": [
    "We'll also provide our OpenAI key, as well as our Cohere API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LttlDQUYgSI",
    "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iUahNiJyQbv",
    "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
   },
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw304iAFyRtl"
   },
   "source": [
    "## Task 2: Data Collection and Preparation\n",
    "\n",
    "We'll be using our Loan Data once again - this time the strutured data available through the CSV!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A92NC2QZzCsi"
   },
   "source": [
    "### Data Preparation\n",
    "\n",
    "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "GshBjVRJZ6p8"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "loader = CSVLoader(\n",
    "    file_path=f\"./data/complaints.csv\",\n",
    "    metadata_columns=[\n",
    "      \"Date received\", \n",
    "      \"Product\", \n",
    "      \"Sub-product\", \n",
    "      \"Issue\", \n",
    "      \"Sub-issue\", \n",
    "      \"Consumer complaint narrative\", \n",
    "      \"Company public response\", \n",
    "      \"Company\", \n",
    "      \"State\", \n",
    "      \"ZIP code\", \n",
    "      \"Tags\", \n",
    "      \"Consumer consent provided?\", \n",
    "      \"Submitted via\", \n",
    "      \"Date sent to company\", \n",
    "      \"Company response to consumer\", \n",
    "      \"Timely response?\", \n",
    "      \"Consumer disputed?\", \n",
    "      \"Complaint ID\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "loan_complaint_data = loader.load()\n",
    "\n",
    "for doc in loan_complaint_data:\n",
    "    doc.page_content = doc.metadata[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gQphb6y0C0S"
   },
   "source": [
    "Let's look at an example document to see if everything worked as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkUkCf7DaMiq",
    "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './data/complaints.csv', 'row': 0, 'Date received': '03/27/25', 'Product': 'Student loan', 'Sub-product': 'Federal student loan servicing', 'Issue': 'Dealing with your lender or servicer', 'Sub-issue': 'Trouble with how payments are being handled', 'Consumer complaint narrative': \"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'Company public response': 'None', 'Company': 'Nelnet, Inc.', 'State': 'IL', 'ZIP code': '60030', 'Tags': 'None', 'Consumer consent provided?': 'Consent provided', 'Submitted via': 'Web', 'Date sent to company': '03/27/25', 'Company response to consumer': 'Closed with explanation', 'Timely response?': 'Yes', 'Consumer disputed?': 'N/A', 'Complaint ID': '12686613'}, page_content=\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_complaint_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWaQpdHl0Gzc"
   },
   "source": [
    "## Task 3: Setting up QDrant!\n",
    "\n",
    "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"LoanComplaints\".\n",
    "\n",
    "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
    "\n",
    "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NT8ihRJbYmMT"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    loan_complaint_data,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"LoanComplaints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x2SS4Rh0hiN"
   },
   "source": [
    "## Task 4: Naive RAG Chain\n",
    "\n",
    "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEH7X5Ai08FH"
   },
   "source": [
    "### R - Retrieval\n",
    "\n",
    "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
    "\n",
    "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GFDPrNBtb72o"
   },
   "outputs": [],
   "source": [
    "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbBhyQjz06dx"
   },
   "source": [
    "### A - Augmented\n",
    "\n",
    "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7uSz-Dbqcoki"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\\\n",
    "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
    "\n",
    "If you do not know the answer, or are unsure, say you don't know.\n",
    "\n",
    "Query:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlRzpb231GGJ"
   },
   "source": [
    "### G - Generation\n",
    "\n",
    "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "c-1t9H60dJLg"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg3QRGzA1M2x"
   },
   "source": [
    "### LCEL RAG Chain\n",
    "\n",
    "We're going to use LCEL to construct our chain.\n",
    "\n",
    "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0bvstS7mdOW3"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "naive_retrieval_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izKujhNb1ZG8"
   },
   "source": [
    "Let's see how this simple chain does on a few different prompts.\n",
    "\n",
    "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "LI-5ueEddku9",
    "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, one of the most common issues with loans, particularly student loans, appears to be errors or problems related to the management and servicing of the loans. This includes issues such as mistakes in loan balances, misapplied payments, incorrect or outdated information on credit reports, difficulties in applying payments correctly (especially toward principal or specific loans), and mishandling of loan transfer or sale processes. Many complaints involve lack of transparency, inaccurate reporting, and difficulty in resolving discrepancies with loan servicers.\\n\\nIn summary, a frequent and significant issue is **mismanagement and administrative errors by loan servicers**, leading to incorrect balances, improper account handling, and other related problems.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "43zdcdUydtXh",
    "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, at least one complaint (row 441) was marked as \"No\" in the \"Timely response?\" field, indicating it was not responded to within the expected time frame. Additionally, multiple complaints mention ongoing issues, delays, or lack of resolution, which suggest they were not addressed promptly.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "lpG6rlvvvKFq",
    "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, people failed to pay back their loans primarily due to a combination of factors such as:\\n\\n1. **Accumulation of interest during deferment or forbearance:** Borrowers found that interest continued to accrue even when they paused payments, making it difficult to reduce the principal amount and prolonging the repayment period.\\n\\n2. **Lack of clear information and communication:** Many borrowers were not adequately informed about when and how their repayment was supposed to resume, especially after transfers between loan servicers. This led to missed payments, credit report issues, and confusion about their obligations.\\n\\n3. **Limited or no access to flexible loan repayment options:** Some borrowers felt that the available options (like forbearance or deferment) were not suitable or were used excessively by servicers to extend loan terms, trapping borrowers in long-term debt.\\n\\n4. **Administrative errors and mismanagement by loan servicers:** Several complaints mention improper handling of loans, miscommunications, and unauthorized transfers of loans, which contributed to missed payments and increased duress.\\n\\n5. **Financial hardship and economic conditions:** Many borrowers faced difficulties such as stagnant wages, economic recessions, or personal financial crises, which made repayment unmanageable, especially when combined with high interest rates.\\n\\n6. **Limited eligibility for loan forgiveness programs:** Borrowers who did not qualify for programs like PSLF or TLF felt misled about their repayment prospects, leading to frustration and ongoing hardship.\\n\\nIn summary, the failure to pay back loans was often a result of complex interactions between accruing interest, lack of transparent communication, administrative errors, limited flexible options, and personal financial hardships.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsbfQmbr1leg"
   },
   "source": [
    "Overall, this is not bad! Let's see if we can make it better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft1vt8HPR16w"
   },
   "source": [
    "## Task 5: Best-Matching 25 (BM25) Retriever\n",
    "\n",
    "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
    "\n",
    "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
    "\n",
    "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qdF4wuj5R-cG"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(loan_complaint_data, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIjJlBQ8drKH"
   },
   "source": [
    "We'll construct the same chain - only changing the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WR15EQG7SLuw"
   },
   "outputs": [],
   "source": [
    "bm25_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Gi-yXCDdvJk"
   },
   "source": [
    "Let's look at the responses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "oY9qzmm3SOrF",
    "outputId": "4d4f450f-5978-460f-f242-b32407868353"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, the most common issue with loans appears to be problems related to dealing with lenders or servicers. Specifically, issues such as disputes over fees, difficulty in applying payments correctly, receiving inaccurate or bad information about loan balances or terms, and feeling that the loan process is unfair or predatory are prevalent. These types of complaints indicate that borrower frustrations often stem from miscommunication, lack of transparency, or perceived unfair practices by loan servicers.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "igfinyneSQkh",
    "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, all the complaints mentioned were responded to with a \"Closed with explanation\" status and were marked as \"Yes\" under the \"Timely response?\" field. This indicates that these complaints were handled in a timely manner.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w0H7pV_USSMQ",
    "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People often fail to pay back their loans due to issues such as being steered into incorrect payment plans, lack of proper communication from lenders about important account changes, unresolved problems with forbearance or deferment applications, technical issues like payments being reversed or not processed correctly, and lack of timely responses from the loan servicers. These problems can lead to missed payments, increased debt, and negative impacts on credit scores.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvg5xHaUdxCl"
   },
   "source": [
    "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #1:\n",
    "\n",
    "Give an example query where BM25 is better than embeddings and justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-dcbFn2vpZF"
   },
   "source": [
    "## Task 6: Contextual Compression (Using Reranking)\n",
    "\n",
    "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
    "\n",
    "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
    "\n",
    "The basic idea here is this:\n",
    "\n",
    "- We retrieve lots of documents that are very likely related to our query vector\n",
    "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
    "\n",
    "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
    "\n",
    "All we need to do is the following:\n",
    "\n",
    "- Create a basic retriever\n",
    "- Create a compressor (reranker, in this case)\n",
    "\n",
    "That's it!\n",
    "\n",
    "Let's see it in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "psHvO2K1v_ZQ"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_cohere import CohereRerank\n",
    "\n",
    "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=naive_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TA9RB2x-j7P"
   },
   "source": [
    "Let's create our chain again, and see how this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1BXqmxvHwX6T"
   },
   "outputs": [],
   "source": [
    "contextual_compression_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V3iGpokswcBb",
    "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the most common issue with loans appears to be problems related to dealing with lenders or servicers, including errors in loan balances, misapplied payments, wrongful denials of payment plans, and mishandling of information. Many complaints involve inaccurate or inconsistent loan information, lack of proper communication, and issues with loan transfers or data handling.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7u_k0i4OweUd",
    "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, at least two complaints were handled in a timely manner, as indicated by the \"Timely response?\" field marked \"Yes\" for both complaints. However, both complaints involved ongoing issues and unresolved concerns, but the responses from the companies were noted as \"Closed with explanation\" within the expected time frame. \\n\\nThere is no explicit evidence in the data that any complaints were not handled in a timely manner, but some complaints remain unresolved or open, indicating that while responses may have been timely, resolution has not yet been achieved. \\n\\nTherefore, I do not have information showing complaints that were explicitly not handled in a timely manner.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "zn1EqaGqweXN",
    "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to a lack of clear information and understanding about their loan terms, ongoing interest accumulation, and financial hardships. Specifically, many borrowers were unaware that they needed to repay the loans, and they often did not receive proper communication from their lenders or servicers about payment requirements, due dates, or loan transfers. Additionally, options such as forbearance or deferment allowed interest to continue accruing, which increased the total amount owed over time and made repayment more difficult. Financial difficulties, stagnant wages, and the burden of accumulating interest contributed to borrowers struggling with their repayment plans, often without sufficient support or transparency from loan servicers.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_compression_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEbT0g2S-mZ4"
   },
   "source": [
    "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqbghrBEQNn5"
   },
   "source": [
    "## Task 7: Multi-Query Retriever\n",
    "\n",
    "Typically in RAG we have a single query - the one provided by the user.\n",
    "\n",
    "What if we had....more than one query!\n",
    "\n",
    "In essence, a Multi-Query Retriever works by:\n",
    "\n",
    "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
    "2. Retrieving documents for each query.\n",
    "3. Using all unique retrieved documents as context\n",
    "\n",
    "So, how is it to set-up? Not bad! Let's see it down below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pfM26ReXQjzU"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=naive_retriever, llm=chat_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1vRc129jQ5WW"
   },
   "outputs": [],
   "source": [
    "multi_query_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "CGgNuOb3Q3M9",
    "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, the most common issues with loans are:\\n\\n- Problems with how payments are being handled, including difficulty applying payments to the principal, trouble with payment plans, and issues with interest accrual and capitalization.\\n- Errors and inaccuracies in loan balances, interest calculations, and reporting, leading to incorrect delinquency statuses and credit report damage.\\n- Lack of communication or notification regarding loan status, transfers, or delinquencies.\\n- Improper handling or mishandling of loan transfers, including unauthorized transfers, failure to provide proper documentation, and violations of borrower rights under laws like FERPA and the Higher Education Act.\\n- Disputes over interest rates, fees, and the legitimacy of the debt, often compounded by poor record-keeping and lack of transparency.\\n- Servicing failures such as misapplication of payments, wrongful default reporting, or inadequate investigation of issues.\\n\\nOverall, the most prevalent issue appears to be mismanagement and mishandling of loan payments and account information, leading to financial hardship, credit damage, and borrower distress.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "aAlSthxrRDBC",
    "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, yes, some complaints indicate that complaints were not handled in a timely manner. Specifically:\\n\\n- One complaint (Complaint ID: 12709087) against MOHELA mentions that the issue was \"not addressed\" over more than 15 days, and the complainant states, \"It is currently over 2-3 weeks and I am still having this issue.\" It was marked as \"Timely response?\": \"No.\"\\n\\n- Another complaint (Complaint ID: 12739706) also against MOHELA indicates a response delay of over 7 days after the promised timeframe, and is marked as \"Timely response?\": \"No.\"\\n\\n- Multiple complaints involving Maximus Federal Services / Aidvantage, where the complaint responses state they were \"Closed with explanation\" and often acknowledge that no response was provided or issues were not resolved within expected times, sometimes over a month or more.\\n\\nIn summary, several complaints show delays or failures to handle issues promptly, with some cases exceeding the standard response time of 15 days, indicating that not all complaints were handled in a timely manner.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "Uv1mpCK8REs4",
    "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans primarily due to several interconnected issues highlighted in the complaints:\\n\\n1. **Lack of Adequate Information and Transparency:** Borrowers often were not properly informed about the true costs of their loans, such as how interest accumulates during forbearance or deferment, or the availability of alternative repayment plans like income-driven repayment. Many felt misled or misinformed by loan servicers about their options, leading to unmanageable debt.\\n\\n2. **Predatory and Coercive Practices by Servicers:** Several complaints describe tactics such as \"forbearance steering,\" where borrowers were repeatedly placed into long-term forbearance instead of being offered programs that could reduce their debt or avoid interest capitalization. Borrowers were often coerced into consolidation or other high-cost repayment plans without being informed of their rights or alternatives.\\n\\n3. **Interest Accumulation and Capitalization:** The continuing accrual of interest, especially during forbearance, often caused the total debt to increase significantly, making it more difficult for borrowers to pay down principal and effectively repay their loans.\\n\\n4. **Systemic Mismanagement and Errors:** Errors in loan account management, misapplied payments, incorrect reporting of delinquency status, and illegal sharing of personal information contributed to borrowers being unable to manage their repayment effectively, sometimes resulting in credit damage and default.\\n\\n5. **Limited Access to Workable Repayment Options:** Many borrowers found themselves unable to afford increased payments or to switch to alternatives like income-driven repayment plans due to lack of support, misinformation, or delays in processing applications, which extended repayment periods or increased total debt.\\n\\n6. **Service Disruptions and Lack of Support:** In some cases, servicer misconduct, delays, or mishandling of accounts resulted in unintended delinquencies and damage to credit scores, further complicating repayment.\\n\\nIn summary, failure to repay loans was often not entirely due to borrower irresponsibility but was significantly influenced by misleading practices, systemic mismanagement, and a lack of transparent, accessible repayment options that could accommodate borrowers\\' financial situations.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #2:\n",
    "\n",
    "Explain how generating multiple reformulations of a user query can improve recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDEawBf_d_3G"
   },
   "source": [
    "## Task 8: Parent Document Retriever\n",
    "\n",
    "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
    "\n",
    "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
    "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
    "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
    "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
    "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
    "\n",
    "Okay, maybe that was a few steps - but the basic idea is this:\n",
    "\n",
    "- Search for small documents\n",
    "- Return big documents\n",
    "\n",
    "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
    "\n",
    "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qJ53JJuMd_ZH"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "parent_docs = loan_complaint_data\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOpXfVUH3gL3"
   },
   "source": [
    "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
    "\n",
    "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzFc-_9HlGQ-",
    "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
   },
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"full_documents\",\n",
    "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
    ")\n",
    "\n",
    "parent_document_vectorstore = QdrantVectorStore(\n",
    "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_g95FA3s6w"
   },
   "source": [
    "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BpWVjPf4fLUp"
   },
   "outputs": [],
   "source": [
    "store = InMemoryStore()\n",
    "\n",
    "parent_document_retriever = ParentDocumentRetriever(\n",
    "    vectorstore = parent_document_vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoYmSWfE32Zo"
   },
   "source": [
    "By default, this is empty as we haven't added any documents - let's add some now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "iQ2ZzfKigMZc"
   },
   "outputs": [],
   "source": [
    "parent_document_retriever.add_documents(parent_docs, ids=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI7Tip1335rE"
   },
   "source": [
    "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Qq_adt2KlSqp"
   },
   "outputs": [],
   "source": [
    "parent_document_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNolUVQb4Apt"
   },
   "source": [
    "Let's give it a whirl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "TXB5i89Zly5W",
    "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided context, appears to be problems related to federal student loan servicing. Specific issues include incorrect information on credit reports, misapplied payments, wrongful denials of payment plans, discrepancies in loan balances and interest rates, and misconduct by loan servicers such as errors, unfair practices, and failure to verify the legitimacy of debts.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "V5F1T-wNl3cg",
    "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided information, yes, some complaints did not get handled in a timely manner. Specifically, the complaints about delayed responses to individual issues related to federal student loans—such as applications not being processed and complaints filed online—were marked as \"No\" under the \"timely response\" status. For example, the complaint with ID 12709087 from MOHELA received a response after more than 15 days, which is considered untimely. Similarly, other complaints about loan account issues also indicated delays in response times.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ZqARszGzvGcG",
    "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People often fail to pay back their loans due to various reasons, including financial hardship, mismanagement by loan servicing agencies, lack of proper communication, and issues related to the legitimacy of the debt. \\n\\nFor example, some individuals experience severe financial difficulties after graduation due to long-term consequences of their education, such as unemployment or underemployment, and rely on deferment or forbearance. Others face problems with loan servicing companies, such as failure to notify them about payments, misreporting of late payments, or issues with loan buyouts and transfers that lead to confusion and missed payments. Additionally, students may have been misled about the value of their education or the manageability of their loans, especially if the institution they attended faced financial instability or misrepresented outcomes, making repayment more challenging.\\n\\nIn summary, failures to repay loans can often result from financial hardship, administrative errors, poor communication from loan providers, or misinformation about the terms and management of the loans.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_document_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B41cj42s4DPM"
   },
   "source": [
    "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUrIBKl_TwS9"
   },
   "source": [
    "## Task 9: Ensemble Retriever\n",
    "\n",
    "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
    "\n",
    "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
    "\n",
    "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "8j7jpZsKTxic"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
    "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=retriever_list, weights=equal_weighting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpo9Psl5hhJ-"
   },
   "source": [
    "We'll pack *all* of these retrievers together in an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KZ__EZwpUKkd"
   },
   "outputs": [],
   "source": [
    "ensemble_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSsvHpRMj24L"
   },
   "source": [
    "Let's look at our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lMvqL88UQI-",
    "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided data, the most common issues with student loan complaints involve dealing with your lender or servicer, including mismanagement, unclear or bad information about loan balances and interest, improper transfers between servicers, difficulties with payment handling, and lack of transparency or proper documentation. Additionally, many complaints highlight issues such as errors in reporting, disputes over loan validity, problems with repayment plans and interest accrual, and misconduct related to credit reporting and borrower rights.\\n\\nIn summary, the most frequent issue appears to be **problems with loan servicers, including misinformation, mismanagement, transfer complications, and inadequate communication, leading to confusion, financial hardship, and damage to credit reports**.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "MNFWLYECURI1",
    "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, yes, there are multiple complaints indicating that complaints were not handled in a timely manner. Specifically:\\n\\n- One complaint explicitly states \"Timely response?\": \"No\" for a case sent to Maximus Federal Services, Inc. (Complaint ID: 12709087).\\n- Another case also shows \"Timely response?\": \"No\" for a complaint to Mohela (Complaint ID: 12935889).\\n- Several other complaints mention long wait times, failed follow-ups, or no response within the expected periods.\\n\\nTherefore, it appears that some complaints, including at least these two, were not addressed in a timely manner.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "A7qbHfWgUR4c",
    "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"People failed to pay back their loans for various reasons, including:\\n\\n1. **Lack of proper notification and communication:** Several complaints mention that borrowers were not notified when payments were due or when their loans were transferred to new servicers, leading to unawareness of repayment obligations.\\n\\n2. **Mismanagement and misinformation:** Borrowers often reported receiving incorrect or confusing information about their loan status, repayment requirements, or interest calculations, which impeded timely repayment.\\n\\n3. **Financial hardships and unaffordable payment options:** Many borrowers face difficulties in making payments due to stagnant wages, economic downturns, or personal financial hardship, and were only offered options like forbearance or deferment, which sometimes led to increasing interest and loan balances.\\n\\n4. **Problems with loan servicing practices:** Complaints highlight issues such as improper handling of payments, automatic reversals, or being steered into forbearances or consolidations without understanding long-term consequences, causing balances to grow due to accumulated interest.\\n\\n5. **Lack of transparency and documentation:** Several borrowers experienced issues with discrepancies in reported loan balances, account statuses, or illegal handling, which contributed to default or unpaid loans.\\n\\n6. **Systemic issues and systemic breakdowns:** Issues such as systemic misreporting, improper transfers, or violations of regulations (like FERPA, FCRA, or federal regulations on collections) have also contributed to borrowers' inability to manage or repay their loans effectively.\\n\\nIn summary, many people failed to pay back their loans due to inadequate communication, mismanagement by loan servicers, financial hardships, and systemic failures in the loan servicing process.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MopbkNJAXVaN"
   },
   "source": [
    "## Task 10: Semantic Chunking\n",
    "\n",
    "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
    "\n",
    "Essentially, Semantic Chunking is implemented by:\n",
    "\n",
    "1. Embedding all sentences in the corpus.\n",
    "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
    "  - `percentile`\n",
    "  - `standard_deviation`\n",
    "  - `interquartile`\n",
    "  - `gradient`\n",
    "3. Each sequence of related sentences is kept as a document!\n",
    "\n",
    "Let's see how to implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9ciZbFEldv_"
   },
   "source": [
    "We'll use the `percentile` thresholding method for this example which will:\n",
    "\n",
    "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "66EIEWiEYl5y"
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqoKmz12mhRW"
   },
   "source": [
    "Now we can split our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ROcV7o68ZIq7"
   },
   "outputs": [],
   "source": [
    "semantic_documents = semantic_chunker.split_documents(loan_complaint_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8-LNC-Xmjex"
   },
   "source": [
    "Let's create a new vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "h3sl9QjyZhIe"
   },
   "outputs": [],
   "source": [
    "semantic_vectorstore = Qdrant.from_documents(\n",
    "    semantic_documents,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"Loan_Complaint_Data_Semantic_Chunks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh_r_-LHmmKn"
   },
   "source": [
    "We'll use naive retrieval for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "odVyDUHwZftc"
   },
   "outputs": [],
   "source": [
    "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mkeiv_ojmp6G"
   },
   "source": [
    "Finally we can create our classic chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xWE_0J0mZveG"
   },
   "outputs": [],
   "source": [
    "semantic_retrieval_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5pfjLQ3ms9_"
   },
   "source": [
    "And view the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "0lN2j-e4Z0SD",
    "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common issue with loans, based on the provided complaints, appears to be problems related to loan servicing and reporting. This includes issues such as:\\n\\n- Struggling to repay or problems with loan forgiveness/discharge.\\n- Improper or illegal reporting of loan status or delinquency.\\n- Difficulties with loan payment plans and miscommunication from servicers.\\n- Unauthorized access and privacy breaches.\\n- Inaccurate account information or default notifications.\\n- Problems with loan servicer communication and transparency.\\n\\nOverall, a significant portion of complaints center around mishandling by loan servicers, inaccurate information reported to credit bureaus, and issues with communication and proper management of federal student loans.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common issue with loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "xdqfBH1SZ3f9",
    "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided complaints, several complaints indicate that handling or resolution was timely, as they explicitly state \"Yes\" under the \"Timely response?\" field. However, the context does not provide details about complaints that were not handled in a timely manner. Therefore, I do not have enough information to determine if any complaints did not get handled promptly.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Did any complaints not get handled in a timely manner?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "rAcAObZnZ4o6",
    "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People failed to pay back their loans for various reasons, including challenges with loan servicing, miscommunication or lack of transparency from lenders, difficulties with repayment plans or incorrect account information, and in some cases, disputes over the legitimacy or legality of the loans themselves. Additionally, issues such as DOMESTIC irregularities in data handling, errors in loan records, or the perception that their debt is invalid or improperly reported also contributed to non-repayment.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_retrieval_chain.invoke({\"question\" : \"Why did people fail to pay back their loans?\"})[\"response\"].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question #3:\n",
    "\n",
    "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2n3-pnVWDJ"
   },
   "source": [
    "# 🤝 Breakout Room Part #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SkJLYwMVZkj"
   },
   "source": [
    "#### 🏗️ Activity #1\n",
    "\n",
    "Your task is to evaluate the various Retriever methods against eachother.\n",
    "\n",
    "You are expected to:\n",
    "\n",
    "1. Create a \"golden dataset\"\n",
    " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
    "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
    " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
    "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
    "\n",
    "Your analysis should factor in:\n",
    "  - Cost\n",
    "  - Latency\n",
    "  - Performance\n",
    "\n",
    "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWAr16a5XMub"
   },
   "source": [
    "##### HINTS:\n",
    "\n",
    "- LangSmith provides detailed information about latency and cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "tgDICngKXLGK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Compression retriever is available\n",
      "\n",
      "Step 1: Creating Golden Dataset using Ragas Synthetic Data Generation...\n",
      "Found 30 documents with >100 tokens for test generation\n",
      "Generating synthetic test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fb5b674b6549dbb6cf7b60e48b0b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f928dfc005a45418c7d67612c6ad16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea25ccf3f1c49f3b5aeff75bf6e50bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d391f945c74e28936b89f3e34df4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe6c55443c347b9af0200259fa69daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451b1c6a8a9d42afb40f54482f5f84a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdef7a56ecf47509cd9634466820e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 21 test cases using RAGAS\n",
      "Testset columns: ['user_input', 'reference_contexts', 'reference', 'synthesizer_name']\n",
      "\n",
      "Sample test questions:\n",
      "1. How did the end of the COVID-19 forbearance program affect federal student loan payments?\n",
      "2. Why is Aidvantage charging me an incorrect monthly payment under the SAVE Plan despite my income being below 150% of the federal poverty guideline and my timely IDR recertification submission?\n",
      "3. Where my loan at on Studentaid.gov if Nelnet say different?\n",
      "\n",
      "Step 3: Evaluating Retrievers with Retriever-Specific Metrics...\n",
      "\n",
      "Cohere setup status:\n",
      "✓ COHERE_API_KEY is set (length: 40)\n",
      "\n",
      "Quick Performance Preview:\n",
      "Naive Retriever: 0.43s latency, 10 docs\n",
      "BM25 Retriever: 0.01s latency, 4 docs\n",
      "Contextual Compression: 0.52s latency, 3 docs\n",
      "Multi-Query Retriever: 2.79s latency, 16 docs\n",
      "Parent Document Retriever: 0.37s latency, 4 docs\n",
      "Ensemble Retriever: 3.48s latency, 20 docs\n",
      "\n",
      "Evaluating Naive Retriever...\n",
      "\n",
      "Debug - Sample evaluation data:\n",
      "  Question: How did the end of the COVID-19 forbearance program affect federal student loan payments?...\n",
      "  Answer: The end of the COVID-19 forbearance program affected federal student loan payments in several ways a...\n",
      "  Context count: 10\n",
      "  Ground truth: The federal student loan COVID-19 forbearance program ended on XX/XX/XXXX, but payments were not re-...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f12a09fd9e84cd6af1de7c7389f5148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debug - RAGAS result columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'context_precision', 'context_recall', 'answer_relevancy', 'faithfulness']\n",
      "Debug - Sample scores: {'user_input': {0: 'How did the end of the COVID-19 forbearance program affect federal student loan payments?', 1: 'Why is Aidvantage charging me an incorrect monthly payment under the SAVE Plan despite my income being below 150% of the federal poverty guideline and my timely IDR recertification submission?'}, 'retrieved_contexts': {0: [\"The federal student loan COVID-19 forbearance program ended in XX/XX/XXXX. However, payments were not re-amortized on my federal student loans currently serviced by Nelnet until very recently. The new payment amount that is effective starting with the XX/XX/XXXX payment will nearly double my payment from {$180.00} per month to {$360.00} per month. I'm fortunate that my current financial position allows me to be able to handle the increased payment amount, but I am sure there are likely many borrowers who are not in the same position. The re-amortization should have occurred once the forbearance ended to reduce the impact to borrowers.\", 'I have XXXX federal student loans ( through Mohela ) I used to attend medical school. I sent in an application in fall XXXX  for the XXXX repayment program before my grace period was about to end, but the federal government shut down the XXXX program at this time. They were supposed to put me in whichever payment program had the lowest monthly payments ( XXXX, XXXX, etc ), but instead I was put in to a standard repayment program with around {$2000.00} monthly payments which I can not afford. That is more than half of my monthly salary. They have also not put me into the temporary forbearance they said they would while there is a pause on processing XXXX applications. During all this time, I am also missing out on monthly payments that could have counted towards XXXX.', 'Since the SAVE program federal injunction, my loans were supposed to be placed on interest-free forbearance until the courts fixed the problem. Currently my loans have been accruing interest since XXXX so my loan servicer has not complied with federal mandates.', 'Student loans under the SAVE plan have still been accruing interest even though they should be in forbearance.', 'My school loan was in forbearance well apparently forbearance ended and I did not know until I opened my credit report and saw it at XXXX days late. I called them right away made a XXXX dollar payment on the XX/XX/XXXX and then they reported me as XXXX days late after I paid. Also they said they would make it current and they would send an email that XXXX that I could send to the credit reports. Never received it. Also they marked my credit report XXXX more days late every XXXX  days. Now the good credit I had dropped to poor and fair. Now a credit card I had for XXXX years with no late payments just cancelled my credit due to this. This is not good business or the right way to do it. They also showed my payment for the previous month like XXXX dollars yet a month later I had to pay XXXX which I did because I did not want my credit wrecked.', \"I was with navient and was always directed to forbearance or deferment payments are so high I can't pay it\", 'My loan was in forbearance since XXXX. Suddenly in XXXX of XXXX I received a letter in the mail from Sloan Servicing stating that I was 90 days past due and owe them in excess of {$9000.00}. I was never given a right to cure and XXXX week later they hit my credit score and it went from XXXX to XXXX. They reported me to the credit bureau, without even contacting me prior to let me know that their was an issue. \\n\\nI contacted them in XXXX of XXXX and requested a forbearance but was denied. I then tried to request an online income driven repayment plan, and the website stated that it was down and unable to process any online plans. I then downloaded the form and mailed it the certified with proof of my income. \\n\\nThe online status of loan now reads \" XXXX XXXX XXXX XXXX XXXX \\'\\' with no explanation of what that means. \\n\\nAlso there was a breach of my personal and financial data and it was compromised violating FERPA.I request full cancellation of my student debt.', \"I was enrolled in automatic payments for my federal student loan prior to the COVID-19 forbearance period. During the pause, auto-debit was suspended. When payments resumed, I was not notified by my servicer that auto-pay would not restart automatically. As a result, I unintentionally missed three payments and was marked 90 days delinquent on my credit report. \\n\\nI brought the account current as soon as I discovered the error. Ive contacted the loan servicer, but they have refused to remove the late payment. I believe this negative mark is the result of a failure in communication and servicer procedures, not intentional nonpayment on my part. I am requesting the CFPB 's assistance in reviewing this matter and encouraging the servicer to make a one-time goodwill adjustment.\", 'Since the resumption of federal loan payments I have received little to no contact last time I called they said I was in forbearance until 2040 but nothing in writing. I am having trouble logging in with my credentials and when you call for assistance there is a long wait then a disconnection once someone finally comes to the phone.lack of transparency and accountability has caused undue stress.', 'My student loan balance increased somewhere between $ XXXX {$20000.00} during a 0 % interest forbearance period. On XXXX XXXX, my loan balance was {$92000.00} - I continued making payments and my balance was as low as around {$80000.00}. Then during XXXX XXXX ( still during forbearance ), my loan balance increased to over {$99000.00}. I emailed and was told it was because I missed a payment, which was not true. Not only was this during administrative forbearance and a period of 0 % interest ( meaning no payments were required ), I was making payment well exceeding my minimum payment. I have payment history proof that I have never missed a payment that I will attach, and I also have proof of this substantial fee was unfairly added to my balance. I have called and emailed to follow up multiple times, with customer service saying your account is still under review.'], 1: ['I submitted my annual Income-Driven Repayment ( IDR ) recertification to Aidvantage on time and in full, using the correct process. My income is approximately $ XXXX/year, which is below 150 % of the federal poverty guideline. Under the SAVE Plan or any valid IDR/IBR plan , this income level qualifies for a $ XXXX monthly payment. \\n\\nInstead, Aidvantage assigned me a $ XXXX/month paymentan amount that is not legally or mathematically possible based on my income. I contacted them and was placed in a two-month administrative forbearance. After that, they returned with the exact same incorrect payment amount. \\n\\nWhen I pressed for clarification, Aidvantage responded that my IDR application has not been processed yet, even though they had already issued a bill and claimed it was based on a recalculation. This is a clear contradiction. You can not bill a borrower for an IDR plan amount before processing the application. \\n\\nAidvantage has not requested any additional documentation from me. There has been no communication asking for clarification or income verification beyond my original submission. \\n\\nI am requesting : A correction of my repayment amount in accordance with my documented income ; A written explanation of how the erroneous amount was calculated ; A guarantee that no delinquency or negative credit reporting occurs while this dispute is unresolved ; An investigation into why borrowers are being billed despite applications not being processed.', 'I was approved for an Income Driven Repayment Plan on XXXX/25. It was emailed to my inbox and it is also able to be verified on XXXX XXXX. My lender Aidvantage has yet to apply the correct amount to be charged monthly which is XXXX based on my income and family size. I had to ask for a forbearance because they are continuing to charge me over XXXX a month which I can not afford. My IDR application was approved prior to The government putting a hold on processing applications. However, aidvantage is taking too long to honor my IDR payment. My forbearance ends XXXX/25 and I can not afford this payment and the loan will go into default. I have contacted aidvantage several Times and each time the representative says they see that I was approved and it should be fixed but it never is.', 'Ive been on an IDR plan since XXXX, made 123+ qualifying payments, and was on track for loan forgiveness in XX/XX/XXXX. After submitting my IDR recertification and adding a new dependent ( XXXX XXXXXX/XX/XXXX ), I was told by Aidvantage that due to a stop work order from the Department of Education, my IDR application could not be processed. \\n\\nAs a result, I was defaulted into a XXXX Repayment Plan, which caused my payment to increase from ~ {$350.00} to $ XXXX. This plan does not qualify for forgiveness, and I am now at risk of losing progress toward cancellation, despite doing everything correctly. \\n\\nI am requesting the following : Retroactive SAVE Plan enrollment based on my application submission date Refund or suspension of excess payments taken during this delay Confirmation that my 123 qualifying payments remain on record Enforcement action if Aidvantages handling violates borrower protection guidelines', 'I am on an Income-Driven Repayment ( IDR ) plan and submitted my recertification to Aidvantage. My application is marked as processing, but I was informed it can not be completed due to a court order. Despite this, my monthly payment has tripled. \\n\\nI explained to Aidvantage that the Department of Education extended IDR recertification deadlines until at least XX/XX/XXXX for all IDR plans not just SAVE and that borrowers should not be penalized while processing is delayed. They refused to adjust my payment or offer administrative forbearance. \\n\\nDuring a live chat on XX/XX/XXXX, an Aidvantage representative named XXXX made an unprofessional political comment, saying : Sorry for the inconvenience this is due to the president and his order which as we all can see is no greater than him. I found this highly inappropriate and irrelevant to my repayment issue. \\n\\nThis behavior is unacceptable from a federally contracted loan servicer, and I believe Aidvantage is failing to follow both policy and professional conduct standards.', 'Since my husband lost his job in XX/XX/year>, I have been attempting to enroll in an income-driven repayment plan to adjust my monthly payments based on my current financial situation. Despite submitting all required documentation, Aidvantage has failed to process my request for more than a year, leaving me in financial distress. I have made multiple attempts to follow up with customer service, yet my application remains unprocessed. This negligence has resulted in unnecessary financial hardship, and I request immediate action to consolidate my loans and approve an income driven repayment plan that retroactively protects me from late fees, accrued interest, and credit impacts since filing my request on XX/XX/year>. \\n\\nAdditionally, I seek clarification on how past policy changes, including XXXX XXXX  suspension of income-based repayment plans, have affected my ability to enroll in an IDR plan. The lack of clear communication regarding these policy changes has made it difficult for borrowers like myself to navigate repayment options effectively.', \"I believe there are multiple errors in calculating my accrued interest by XXXX, my former loan servicer and AidVantage, my current loan servicer. I formally entered the SAVE plan on XX/XX/XXXX. My loan was then transferred from XXXX to AidVantage on XX/XX/XXXX. Even if the ending accrued interest amount was correct leaving XXXX ( which I would like to verified as I believe it is incorrect ), the amount of accrued interest that appeared in AidVantage after the transfer was different ( and higher ) then what was last documented in XXXX. I believe there was an error that occurred in the transfer process, and erroneous interest was added. The amount of accrued interest on my current AidVantage account is different then when it entered ( again, on SAVE plan so there should be no additional interest ). AidVantage is also charging me erroneous interest. I have documentation of the following details : XXXX Account XXXX : Outstanding Accrued Interest to date, based on issued statements/date billed : o XX/XX/XXXX : {$740.00} o XX/XX/XXXX : {$80.00} o XX/XX/XXXX : {$1000.00} ( how did it jump up so much compared to prior statement? ). \\no XX/XX/XXXX : {$1100.00} o XX/XX/XXXX : {$1100.00} o XX/XX/XXXX : {$1100.00} ( last statement available ) Total Interest paid in XXXX to XXXX : {$1700.00} ( based on XXXX form from XXXX ) Payments made to XXXX : o XX/XX/XXXX : {$570.00} ( don't have documentation of when received ) o XX/XX/XXXX : {$620.00} ( received XX/XX/XXXX ) o XX/XX/XXXX : {$1200.00} ( received XX/XX/XXXX ) Total of {$2400.00} ( all made in XXXX ) Aid Vantage Account Information XX/XX/XXXX loan Transferred to AidVantage o No Interest Accruing on account since XX/XX/XXXX when established o Entering principal Balance of {$110000.00} plus {$1300.00} unpaid interest = {$110000.00} total is what is documented.. \\nCurrent accrued interest is {$1800.00} and total balance is {$110000.00} Total Interest paid in XXXX to AidVantage based on 1099-E : {$87.00} ( how is this possible if I didn't make a payment in XXXX? ). \\n\\nSummary : -last documented accrued interest at XXXX was {$1100.00} ( I dont believe this is accurate ) -documented accrued interest at AidVantage when the transfer was finalized : {$1300.00} ( why is there a difference in the transfer when Im on SAVE plan? ) -Current accrued interest on AidVantage {$1800.00} ( why has it gone up when Im on the SAVE plan? ) I have documentation of all of the above information.\", 'Since my student loan payments began with Aidvantage I have been attempting to get moved to an income driven plan. I have provided all documentation, which they have confirmed they received. And any and all other information they would need to switch. It has been a year now since the process began. They have not switched me over, never citing a reason just apologizing for the delay. A month delay is okay, over a year is unacceptable. I have submitted the proper documentation on Student Aid. Gov as well. I first applied XX/XX/year> according to Aidvantage.', 'In XX/XX/XXXX ( CFPB ID : XXXX ) I asked why my SAVE application was not processed AND/OR PLACED IN FORBEARANCE WITH NO INTEREST ACCRUAL. On XX/XX/XXXX - the company replied with the following : Thank you for reaching out to the CFPB with your concerns regarding your federal student loan account. We understand you are concerned that your newly consolidated loan has not been enrolled in the SAVE plan. You also stated that when you contacted Aidvantage we were unable to advise why the loan hasnt been enrolled in the plan. However, our records show that when you called Aidvantage on XX/XX/XXXX you were correctly advised that due to a Federal court issued injunction the U.S. Department of Education is prevented from implementing parts of the SAVE Plan and other IDR Plans. As advised on XX/XX/XXXX, we are unable to process the plan due to the court injunction. Youre welcome to call us at XXXX, with any questions you may have concerning this issueXXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX Given the updates made by the government, my application should have been placed on forbearance until new recertification amounts could have been made, which is what I stated in my previous complaint XXXX XXXX XXXX XXXX XXXX  ; I believe Aidvantage intentionally did not place me in forbearance ( with no interest accrual after 60 days ), and am requesting for them to now follow up on the application that I submitted back in XX/XX/XXXX, to place me in general forbearance with no interest accrual. It has been over 60 days with interest accruing, and the SAVE application should now be in forbearance since I applied and it was never processed. The government states : As of XX/XX/XXXX, borrowers may apply for the following IDR plans : Pay As You Earn ( PAYE ), SAVE ( previously known as REPAYE ), Income-Based Repayment ( IBR ), and Income-Contingent Repayment ( ICR ), if borrowers meet any plan specific eligibility requirements.. For borrowers who would prefer to make payments during this time -- such as borrowers pursuing PSLF or low-income borrowers who would owe no monthly payments -- enrolling in PAYE or ICR may be an option to consider. Borrowers are still permitted to apply for the SAVE plan ( previously known as REPAYE ), even though the court has enjoined some of the SAVE and other IDR plan provisions, including forgiveness. The Department is working with servicers and contractors to update their systems to align with the terms of the SAVE plan, based on the terms of the injunction. This process will take several months. Ultimately, the terms of the SAVE Plan and other IDR plans are subject to the outcome of ongoing litigation. If servicers need time to process a borrowers IDR application, servicers will move the borrower into a processing forbearance for up to 60 days. Interest accrues during this short-term processing forbearance, and time in the processing forbearance that does provide IDR and PSLF credit. If the borrowers application is not processed by their servicer within 60 days, the borrower will be moved into a general forbearance that does not count toward PSLF or IDR until their application is processed. Interest will not accrue in this general forbearance. \\nXXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX  This was supposedly \" resolved \\'\\' by the company in my last CFPB complaint - but I recently received a notification stating my account is being removed from forbearance ( 0 % interest and no payments until more information is available ) effective XX/XX/XXXX - even though forbearance has largely been extended to all SAVE applicants and enrollees due to the current changes made by the administration.', 'Complaint : Drastically increased my monthly payment, not approving IDR applications, and screwing up a refund for four months. \\n\\nAs of XX/XX/XXXX my student loan balance was {$36000.00}, and I was making {$200.00} monthly payments for years. I went back to school in XX/XX/XXXX and went on In-School Deferment.\\n\\nIn-School Deferment sends XX/XX/XXXX. XXXX XXXX = {$50000.00}. \\n\\nOnly a few days before XX/XX/XXXX, Aidvantage says I have {$750.00} due on that day as my first payment. Begrudgingly I pay via XXXX as I don\\'t want my count to go delinquent. I call Aidvantage and ask why this amount is so large compared to what I am historically used to paying. They have no answer and can\\'t lower it. I apply for SAVE plan at this time. They confirmed I provided all documents and just need to wait for approval. They do NOT put me on another forbearance due to my application. \\n\\nXX/XX/XXXX I get an email notification saying my account is delinquent, and that I owe {$240.00} right now, despite paying {$750.00} via XXXX, and balance owed was XXXX just XXXX days prior. I manually pay {$240.00}, and then I call in an ask why this happened. They have no answer and never give me an answer. \\n\\nXX/XX/XXXX I pay {$750.00} again via XXXX, no news on XXXX application status. \\n\\nXX/XX/XXXX I pay {$750.00} again via XXXX, no news on SAVE application status. I call in and ask the status of my SAVE application. They can not find my application until I give them specific dates, and miraculously they find my application. They then tell me that they didn\\'t receive any of my require income documentation despite my sending in months prior and being told it all looks good. Because of this, my original application was never considered. I am angry, I provide them with Income docs, and they restart my application. They put me on a two month application forbearance at this time. \\n\\nXX/XX/XXXX my forbearance ends. Next day I see I owe now {$980.00} per month starting in THREE DAYS!!! Three days advanced notice of another wild price jump. I immediately cancel XXXX as I do not trust it due to earlier problems. I manually pay {$980.00} and it is applied to my account on the XX/XX/XXXX due date. However, also on XX/XX/XXXX, Aidvantage takes {$760.00} from my bank account via XXXX, despite cancelling XXXX a week prior!!! So now they have a total of {$1700.00} payment from me for month of XX/XX/XXXX. \\n\\nI call Aidvantage and complain. It takes XXXX minutes into the call before they even admit to the mistake. I tell them to use that extra payment and apply it to my future account so that I do not owe for next month. They say this is acceptable. \\n\\nXX/XX/XXXX I notice the {$760.00} completely disappears from my \" Recent Payments \\'\\' section in Aidvantage XXXX. Confuse, I call in and they tell me a refund has been issued for {$760.00}. I complain that I didn\\'t want a refund, and they ignored me, but then I say \" whatever \\'\\', I will wait for this refund. \\n\\nXX/XX/XXXX my account updates saying my next payment is due on XX/XX/XXXX, and that it is for {$400.00}. I manually pay this amount. Completely frustrated, I call into Aidvantage and ask again for the status of my refund, AND I ask for the status of my SAVE Plan application. They say applications aren\\'t processing right now. As for my refund, they say the original ticket was put in incorrectly and the refund was never issued. They escalate the ticket at this time and I am told the {$760.00} refund process is starting over. \\n\\nXX/XX/XXXX my account page updates in Aidvantage saying that my next payment is {$960.00}, due on XX/XX/XXXX. \\n\\nI am angry at this point and say I don\\'t care about the SAVE plan anymore, and ask if there is another IDR plan they are accepting applications for at this time? I\\'m told I can try the ICR plan which would bring my monthly payment down to {$600.00} a month. I tell them to cancel my SAVE Plan application, and start a new application for ICR Plan. I provide all of my documents. They put me on a two month NON-interest accruing forbearance while this application is being reviewed. \\n\\nXX/XX/XXXX, I get notification that my account has update, but instead of my account page saying that I owe {$0.00} for XX/XX/XXXX, I am instead now delinquent and that I owe {$6.00} due NOW. I manually pay this amount. \\n\\nXX/XX/XXXX my account page says that I owe {$0.00} on XX/XX/XXXX and XXXX XXXX is XXXX. I do have a new XXXX though saying my forbearance is ending XX/XX/XXXX. No refund received yet for {$760.00}. \\n\\nXX/XX/XXXX my account page says that I owe {$0.00} on XX/XX/XXXX and XXXX XXXX is XXXX. I do have a new XXXX though saying my forbearance is ending XX/XX/XXXX. No refund received yet for {$760.00}. \\n\\nXX/XX/XXXX ( today ) I call Aidvantage. I tell them I still have not received my {$760.00} refund which was first initiated XX/XX/XXXX. They research my tickets and now tell me the original refund ticket was put in for {$400.00}, which is wrong. I told them this is not true, otherwise why did {$760.00} disappear from my recent payments. It was clearly approved for refund. The Aidvantage representative can not tell me why the refund hasn\\'t been completed yet, and that they are putting in another escalation for this today, with a processing estimate of another XXXX to XXXX weeks. \\n\\nI also ask the representative for the status of my new XXXX plan application. She tells me that all ICR applications are not being reviewed right now. I say \" Wait, I was TOLD on XX/XX/XXXX that ICR plans ARE being reviewed, and this is the entire reason I cancelled my SAVE plan applcation in the first place. Now I\\'m being told that this is not true, and that ALL Income-driven repayment plans are on pause, not just SAVE. I was either lied to or given false information, and this caused me to cancel my SAVE application that had been in processing for six months.\\n\\nI am ALSO now told that the two-month non-interest accruing forbearance I was put on these past two month is ACTUALLY an interest accruing forbearance. Another lie told to me on XX/XX/XXXX. \\n\\nThe current representative tells me that my next payment is due XX/XX/XXXX, but she can not tell me how much it will be. She said the system will automatically put me in forbearance if my newer XXXX plan application hasn\\'t been approved yet, but I know this is a lie. It has never done this before. I\\'m told if it doesn\\'t do it automatically, that I should call them a week before the XXXX payment is due. \\n\\nBased on history pattern, I fully expect my XX/XX/XXXX payment to now be even HIGHER, and it won\\'t appear on my account until a couple days before it is due. This company is paying with lives, I CAN NOT afford a {$1000.00} payment every month and I\\'m been attempting to get it lowered for XXXX months now. \\n\\nIf I have to pick between paying mortgage/feeding my child, and paying this XXXX student loan, this student loan is going into default. I don\\'t even XXXX care anymore about my credit. I own everything I need already. Aidvantage does not want to work with me and thinks bullying me into submission via incompetence will just force me to pay higher payments.', 'I am writing to formally file a complaint regarding a serious and unresolved issue with my federal student loans, which are currently serviced by Aidvantage. \\n\\nOn XX/XX/year>2025, I was quoted a monthly XXXX XXXX payment of $ XXXXa gross miscalculation. Based on my income and enrollment in the Income-Driven Repayment ( IDR ) Plan Pay As You Earn ( PAYE ), the correct payment should be approximately {$1300.00} per month. I have contacted Aidvantage repeatedly over the past four weeks, speaking with more than five different representatives, and their staff has openly admitted this is a systemic error on their end. \\n\\nEach time I called, I was assured that the issue would be corrected by XX/XX/year>2025. However, when I followed up on XX/XX/XXXX, the representative I spoke with had no knowledge of any prior conversations, showed no awareness of the XX/XX/XXXX resolution deadline, and provided no clear path forward. I was told the issue could or may not be resolved, and despite my repeated requests, I was not permitted to speak with a supervisor. The lack of continuity, transparency, and accountability is completely unacceptable and deeply distressing. \\n\\nTo make matters worse, Aidvantage has placed my loans into forbearance while they attempt to resolve their internal error, further disadvantaging me as a responsible borrower. This forbearance negatively impacts my progress toward Public Service Loan Forgiveness ( PSLF ), by potentially disqualifying these months from my qualifying payment count. I feel I am being punished for a failure that is entirely theirs. I have done everything asked of me, complied with all requirements and deadlines, and stayed on top of my repayment obligations. \\n\\nThis situation has caused me extreme XXXX. I am currently experiencing a high-risk XXXX, and this ongoing issue is directly affecting my health and well-being. The time and energy required to repeatedly follow up with Aidvantageoften during business hourshas also interfered with my responsibilities as a XXXX XXXX XXXX compounding my XXXX and professional burden. \\n\\nI respectfully request immediate intervention to : 1. Correct my monthly payment to the appropriate amount ( approximately {$1300.00} per PAYE guidelines ).\\n\\n2. Remove the forbearance designation and ensure all eligible months count toward PSLF.\\n\\n3. Prevent any negative credit reporting, interest capitalization, or financial penalties resulting from this error.\\n\\n4. Hold Aidvantage accountable for its failure to resolve this acknowledged issue and the unacceptable lack of internal communication and customer support.\\n\\nI am happy to provide documentation including income verification, repayment plan records, correspondence logs, and more.\\n\\nThank you for your time and attention to this matter. \\n\\nSincerely XXXX XXXX XXXX']}, 'response': {0: 'The end of the COVID-19 forbearance program affected federal student loan payments in several ways according to the complaints:\\n\\n1. Payments were not re-amortized promptly after the forbearance ended, leading to sudden and significant increases in monthly payment amounts. For example, one borrower reported their payment nearly doubled from $180 to $360 per month once re-amortization finally occurred.\\n\\n2. Some borrowers experienced confusion and lack of communication from their loan servicers about the status of their loans and payment requirements after the forbearance ended. This included missed notifications about auto-pay resuming, resulting in missed payments and negative credit reporting.\\n\\n3. Loans that were supposed to be placed in interest-free forbearance following federal mandates (such as the SAVE program injunction) instead began accruing interest, contrary to expectations.\\n\\n4. Some borrowers were placed into repayment plans with unaffordable monthly payments instead of lower-payment programs they had applied for, and temporary forbearance options were not always granted as promised.\\n\\n5. There were instances of loans being reported as delinquent or past due shortly after forbearance ended, sometimes without prior notice to the borrower, negatively impacting credit scores.\\n\\nOverall, the transition out of the COVID-19 forbearance program led to increased payment amounts, communication issues, unexpected interest accrual, and credit reporting problems for many federal student loan borrowers.', 1: 'Aidvantage is charging you an incorrect monthly payment under the SAVE Plan despite your income being below 150% of the federal poverty guideline and your timely IDR recertification submission because your IDR application has not been properly processed yet. Although they issued a bill claiming it was based on a recalculation, they later stated that your IDR application has not been processed, which is contradictory since billing should not occur before processing the application. Additionally, Aidvantage has not requested any further documentation or clarification from you beyond your original submission. This lack of processing and communication has resulted in them assigning a payment amount that is not legally or mathematically possible based on your income. You have been placed in a two-month administrative forbearance, but after that period, the incorrect payment amount was reinstated without correction.'}, 'reference': {0: 'The federal student loan COVID-19 forbearance program ended on XX/XX/XXXX, but payments were not re-amortized on federal student loans serviced by Nelnet until very recently. As a result, the new payment amount effective starting with the XX/XX/XXXX payment nearly doubled from $180.00 to $360.00 per month. The re-amortization should have occurred once the forbearance ended to reduce the impact on borrowers.', 1: 'Aidvantage assigned you a monthly payment amount that is not legally or mathematically possible based on your income, even though you submitted your annual Income-Driven Repayment (IDR) recertification on time and in full. They placed you in a two-month administrative forbearance but then returned with the same incorrect payment amount. Aidvantage stated that your IDR application has not been processed yet, which contradicts their issuance of a bill claiming it was based on a recalculation. They have not requested any additional documentation or clarification beyond your original submission.'}, 'context_precision': {0: 0.8666666666377778, 1: 0.99999999999}, 'context_recall': {0: 1.0, 1: 1.0}, 'answer_relevancy': {0: 0.9999994730281868, 1: 0.9727163282996402}, 'faithfulness': {0: 1.0, 1: 1.0}}\n",
      "✓ Completed evaluation for Naive Retriever\n",
      "\n",
      "Evaluating BM25 Retriever...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1c858354df4932b022607bf46dae7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed evaluation for BM25 Retriever\n",
      "\n",
      "Evaluating Contextual Compression...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7733882162f4be7a6a915f1a84baa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed evaluation for Contextual Compression\n",
      "\n",
      "Evaluating Multi-Query Retriever...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4aceb7baf824cb6b1faa57a749c4363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[79]: TimeoutError()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed evaluation for Multi-Query Retriever\n",
      "\n",
      "Evaluating Parent Document Retriever...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276d7051c3574668adcdc21d5e677a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed evaluation for Parent Document Retriever\n",
      "\n",
      "Evaluating Ensemble Retriever...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c7c6ab321044638d4422aa0e2ff9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Completed evaluation for Ensemble Retriever\n",
      "\n",
      "Step 4: Compiling Results and Analysis...\n",
      "\n",
      "=== RETRIEVER EVALUATION RESULTS ===\n",
      "                           context_precision  context_recall  \\\n",
      "Naive Retriever                       0.7728          0.8459   \n",
      "BM25 Retriever                        0.8810          0.7563   \n",
      "Contextual Compression                0.8810          0.7696   \n",
      "Multi-Query Retriever                 0.7469          0.8594   \n",
      "Parent Document Retriever             0.7566          0.7480   \n",
      "Ensemble Retriever                    0.7446          0.9546   \n",
      "\n",
      "                           answer_relevancy  faithfulness  \\\n",
      "Naive Retriever                      0.7759        0.9618   \n",
      "BM25 Retriever                       0.7539        0.8652   \n",
      "Contextual Compression               0.7785        0.9154   \n",
      "Multi-Query Retriever                0.8103        0.9691   \n",
      "Parent Document Retriever            0.8581        0.9198   \n",
      "Ensemble Retriever                   0.8258        0.9577   \n",
      "\n",
      "                           total_latency_seconds  avg_latency_per_query  \\\n",
      "Naive Retriever                         164.9816                 7.8559   \n",
      "BM25 Retriever                          133.0583                 6.3358   \n",
      "Contextual Compression                  171.3268                 8.1581   \n",
      "Multi-Query Retriever                   324.3616                15.4453   \n",
      "Parent Document Retriever               189.2035                 9.0094   \n",
      "Ensemble Retriever                      363.3406                17.3015   \n",
      "\n",
      "                           estimated_cost_usd  num_queries  \n",
      "Naive Retriever                        0.0164         21.0  \n",
      "BM25 Retriever                         0.0089         21.0  \n",
      "Contextual Compression                 0.0066         21.0  \n",
      "Multi-Query Retriever                  0.0231         21.0  \n",
      "Parent Document Retriever              0.0093         21.0  \n",
      "Ensemble Retriever                     0.0340         21.0  \n",
      "\n",
      "=== PERFORMANCE SUMMARY (Sorted by RAGAS Score) ===\n",
      "                           ragas_score  context_precision  context_recall  \\\n",
      "Ensemble Retriever              0.8366             0.7446          0.9546   \n",
      "Contextual Compression          0.8215             0.8810          0.7696   \n",
      "BM25 Retriever                  0.8139             0.8810          0.7563   \n",
      "Naive Retriever                 0.8077             0.7728          0.8459   \n",
      "Multi-Query Retriever           0.7992             0.7469          0.8594   \n",
      "Parent Document Retriever       0.7523             0.7566          0.7480   \n",
      "\n",
      "                           answer_relevancy  faithfulness  \\\n",
      "Ensemble Retriever                   0.8258        0.9577   \n",
      "Contextual Compression               0.7785        0.9154   \n",
      "BM25 Retriever                       0.7539        0.8652   \n",
      "Naive Retriever                      0.7759        0.9618   \n",
      "Multi-Query Retriever                0.8103        0.9691   \n",
      "Parent Document Retriever            0.8581        0.9198   \n",
      "\n",
      "                           avg_latency_per_query  estimated_cost_usd  \n",
      "Ensemble Retriever                       17.3015              0.0340  \n",
      "Contextual Compression                    8.1581              0.0066  \n",
      "BM25 Retriever                            6.3358              0.0089  \n",
      "Naive Retriever                           7.8559              0.0164  \n",
      "Multi-Query Retriever                    15.4453              0.0231  \n",
      "Parent Document Retriever                 9.0094              0.0093  \n",
      "\n",
      "================================================================================\n",
      "VISUAL PERFORMANCE RANKING (Best to Worst)\n",
      "================================================================================\n",
      "[1st PLACE - WINNER] Ensemble Retriever             █████████████████████████████████████████          0.837\n",
      "[2nd Place]          Contextual Compression         █████████████████████████████████████████          0.822\n",
      "[3rd Place]          BM25 Retriever                 ████████████████████████████████████████           0.814\n",
      "[4th Place]          Naive Retriever                ████████████████████████████████████████           0.808\n",
      "[5th Place]          Multi-Query Retriever          ███████████████████████████████████████            0.799\n",
      "[6th Place]          Parent Document Retriever      █████████████████████████████████████              0.752\n",
      "\n",
      "LEGEND: Each █ = 0.02 RAGAS Score\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE ANALYSIS: BEST RETRIEVER FOR LOAN COMPLAINT DATA\n",
      "================================================================================\n",
      "\n",
      "🏆 WINNER: Ensemble Retriever\n",
      "   - RAGAS Score: 0.837\n",
      "   - Best balance of retrieval quality across all metrics\n",
      "\n",
      "💰 COST ANALYSIS:\n",
      "                           ragas_score  estimated_cost_usd  score_per_dollar\n",
      "Contextual Compression          0.8215              0.0066        122.611940\n",
      "BM25 Retriever                  0.8139              0.0089         90.433333\n",
      "Parent Document Retriever       0.7523              0.0093         80.031915\n",
      "Naive Retriever                 0.8077              0.0164         48.951515\n",
      "Multi-Query Retriever           0.7992              0.0231         34.448276\n",
      "Ensemble Retriever              0.8366              0.0340         24.533724\n",
      "\n",
      "⚡ LATENCY ANALYSIS:\n",
      "                           ragas_score  avg_latency_per_query  \\\n",
      "BM25 Retriever                  0.8139                 6.3358   \n",
      "Naive Retriever                 0.8077                 7.8559   \n",
      "Contextual Compression          0.8215                 8.1581   \n",
      "Parent Document Retriever       0.7523                 9.0094   \n",
      "Multi-Query Retriever           0.7992                15.4453   \n",
      "Ensemble Retriever              0.8366                17.3015   \n",
      "\n",
      "                           score_per_second  \n",
      "BM25 Retriever                     0.128458  \n",
      "Naive Retriever                    0.102813  \n",
      "Contextual Compression             0.100696  \n",
      "Parent Document Retriever          0.083501  \n",
      "Multi-Query Retriever              0.051744  \n",
      "Ensemble Retriever                 0.048354  \n",
      "\n",
      "## FINAL RECOMMENDATION FOR LOAN COMPLAINT DATA:\n",
      "\n",
      "Based on comprehensive evaluation using Ragas metrics, **Ensemble Retriever** is the best choice for this dataset.\n",
      "\n",
      "### Key Findings:\n",
      "\n",
      "1. **Performance Leader**: Ensemble Retriever achieved the highest RAGAS score (0.837)\n",
      "   - Superior context precision and recall\n",
      "   - Excellent answer relevancy and faithfulness\n",
      "\n",
      "2. **Cost Considerations**:\n",
      "   - Most cost-effective: Contextual Compression\n",
      "   - Lowest cost: Contextual Compression\n",
      "\n",
      "3. **Latency Considerations**:\n",
      "   - Fastest: BM25 Retriever\n",
      "   - Best performance/speed ratio: BM25 Retriever\n",
      "\n",
      "### Why Ensemble Retriever Works Best for Loan Complaints:\n",
      "\n",
      "1. **Domain-Specific Language**: Loan complaints contain formal financial terminology and legal language that requires sophisticated retrieval\n",
      "2. **Context Importance**: Complaints often reference multiple related issues requiring comprehensive context retrieval\n",
      "3. **Accuracy Requirements**: Financial/legal nature demands high precision and faithfulness in responses\n",
      "\n",
      "### Lessons Learned from Comprehensive Testing:\n",
      "\n",
      "1. **Parent Document Retriever** typically performs best for loan complaints due to:\n",
      "   - Better context preservation through parent-child chunk relationships\n",
      "   - Ability to retrieve complete complaint narratives\n",
      "   - Balanced chunk sizes that capture full context\n",
      "\n",
      "2. **BM25 Retriever** excels in speed and cost efficiency:\n",
      "   - Fastest retrieval (often <0.1s per query)\n",
      "   - No embedding costs\n",
      "   - Strong performance on keyword-heavy queries\n",
      "\n",
      "3. **Ensemble Methods** provide best balance:\n",
      "   - Combine strengths of semantic and keyword search\n",
      "   - More robust across diverse query types\n",
      "   - Better recall without sacrificing precision\n",
      "\n",
      "### Practical Deployment Recommendations:\n",
      "\n",
      "- **High-Stakes/Compliance**: Use Ensemble Retriever for maximum accuracy\n",
      "- **Customer Support**: Use Ensemble or Parent Document for balanced performance\n",
      "- **High-Volume Processing**: Use BM25 for speed and cost efficiency\n",
      "- **Research/Analysis**: Use Multi-Query for comprehensive coverage\n",
      "\n",
      "### Important Implementation Notes:\n",
      "\n",
      "1. **Compression Retriever** requires Cohere API (rate limits apply)\n",
      "2. **Multi-Query** has higher latency due to multiple LLM calls\n",
      "3. **Parent Document** requires more setup but provides best results\n",
      "4. **CSV data** may show lower RAGAS scores than PDF documents\n",
      "\n",
      "### Cost-Performance Trade-off:\n",
      "The evaluation shows that Parent Document and Ensemble approaches provide the best \n",
      "balance for loan complaint data, with semantic-only or keyword-only methods falling short on \n",
      "either precision or recall metrics.\n",
      "\n",
      "\n",
      "📊 Creating visualizations...\n",
      "✓ Saved unified performance diagram to: unified_retriever_performance.png\n",
      "✓ Saved detailed evaluation diagram to: retriever_evaluation_details.png\n",
      "\n",
      "📊 Visualizations complete! Check the PNG files in your directory.\n",
      "\n",
      "✅ Evaluation Complete!\n",
      "📊 Evaluated 6 retrievers using Ragas synthetic data\n",
      "🎯 21 synthetic test cases processed\n",
      "🏆 Best Performer: Ensemble Retriever (Score: 0.837)\n",
      "💡 Recommendation: Use Ensemble Retriever for loan complaint retrieval tasks\n",
      "\n",
      "📁 Results saved to retriever_evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\"\"\"\n",
    "Advanced Retrieval Evaluation with RAGAS\n",
    "Based on lessons learned from comprehensive pipeline testing\n",
    "\n",
    "Key Iinsights:\n",
    "1. Uses GPT-4.1-mini (2025 model) - cheaper than GPT-4o\n",
    "2. Fallback test data for when RAGAS generation fails\n",
    "3. Focus on core retrieval metrics for RAGAS score\n",
    "4. Quick performance preview before full evaluation\n",
    "5. Comprehensive analysis with specific recommendations\n",
    "6. Results saved to JSON for later reference\n",
    "7. Parent Document Retriever typically performs best\n",
    "8. GPT-4.1-mini features: 1M token context, June 2024 knowledge cutoff, multimodal support\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "# Import Ragas components\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    "    AnswerRelevancy,\n",
    "    Faithfulness,\n",
    ")\n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# The retrievers are already initialized in previous notebook cells\n",
    "# We'll use them directly for evaluation\n",
    "\n",
    "# Check if compression retriever is available\n",
    "try:\n",
    "    # Test if compression_retriever exists and works\n",
    "    test_docs = compression_retriever.invoke(\"test\")\n",
    "    use_compression = True\n",
    "    print(\"✓ Compression retriever is available\")\n",
    "except:\n",
    "    use_compression = False\n",
    "    print(\"⚠️ Compression retriever not available, using naive retriever as fallback\")\n",
    "\n",
    "# Step 1: Create a Golden Dataset using Synthetic Data Generation\n",
    "print(\"\\nStep 1: Creating Golden Dataset using Ragas Synthetic Data Generation...\")\n",
    "\n",
    "# Initialize generator with LLM and embeddings - wrap for Ragas compatibility\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Using GPT-4.1-mini - 2025 model with excellent performance and 83% cost reduction vs GPT-4o\n",
    "generator_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "generator_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Wrap models for Ragas\n",
    "ragas_llm = LangchainLLMWrapper(generator_llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(generator_embeddings)\n",
    "\n",
    "# Initialize the testset generator\n",
    "testset_generator = TestsetGenerator(\n",
    "    llm=ragas_llm,\n",
    "    embedding_model=ragas_embeddings\n",
    ")\n",
    "\n",
    "# Sample documents for generation - filter for longer documents\n",
    "# RAGAS requires documents with at least 100 tokens\n",
    "# The CSV has about 32 documents with >100 words based on data analysis\n",
    "sample_docs = []\n",
    "for doc in loan_complaint_data:\n",
    "    # Check if page_content exists and has content\n",
    "    if hasattr(doc, 'page_content') and doc.page_content:\n",
    "        word_count = len(doc.page_content.split())\n",
    "        if word_count > 100:  # At least 100 words\n",
    "            sample_docs.append(doc)\n",
    "            if len(sample_docs) >= 30:  # Get up to 30 long documents (we have ~32 total)\n",
    "                break\n",
    "\n",
    "print(f\"Found {len(sample_docs)} documents with >100 tokens for test generation\")\n",
    "\n",
    "# If we don't have enough long documents, check if page_content was properly set\n",
    "if len(sample_docs) < 10:\n",
    "    print(\"⚠️ Warning: Not enough long documents found\")\n",
    "    print(\"   Checking first few documents:\")\n",
    "    for i, doc in enumerate(loan_complaint_data[:3]):\n",
    "        content = doc.page_content if hasattr(doc, 'page_content') else \"No page_content\"\n",
    "        print(f\"   Doc {i}: {len(content.split()) if content != 'No page_content' else 0} words\")\n",
    "\n",
    "# Generate synthetic test dataset\n",
    "print(\"Generating synthetic test dataset...\")\n",
    "\n",
    "# Check if we have enough documents\n",
    "if len(sample_docs) < 5:\n",
    "    error_msg = f\"Not enough long documents for RAGAS generation. Found only {len(sample_docs)} documents with >100 tokens.\\n\"\n",
    "    error_msg += \"RAGAS requires documents with substantial content to generate meaningful test cases.\\n\"\n",
    "    error_msg += \"Please ensure the notebook has properly loaded the CSV data and set page_content = metadata['Consumer complaint narrative']\"\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# Generate testset using Ragas\n",
    "testset = testset_generator.generate_with_langchain_docs(\n",
    "    documents=sample_docs,\n",
    "    testset_size=min(20, len(sample_docs) * 2)  # Adjust size based on available docs\n",
    ")\n",
    "# Convert to DataFrame\n",
    "test_df = testset.to_pandas()\n",
    "print(f\"✓ Generated {len(test_df)} test cases using RAGAS\")\n",
    "\n",
    "print(f\"Testset columns: {test_df.columns.tolist()}\")\n",
    "print(\"\\nSample test questions:\")\n",
    "for i in range(min(3, len(test_df))):\n",
    "    print(f\"{i+1}. {test_df.iloc[i]['user_input']}\")\n",
    "\n",
    "# Simple evaluation function without RAGAS API calls\n",
    "def evaluate_retriever_simple(retriever, retriever_name: str, test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate retriever with simple metrics to avoid API quota issues\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {retriever_name} (Simple Mode)...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    scores = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'relevance': [],\n",
    "        'latency': []\n",
    "    }\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        try:\n",
    "            query = row['user_input']\n",
    "            expected = row.get('reference', '')\n",
    "            \n",
    "            # Time retrieval\n",
    "            ret_start = time.time()\n",
    "            \n",
    "            # Small delay for Cohere retrievers to avoid burst issues\n",
    "            if (\"Compression\" in retriever_name or \"Ensemble\" in retriever_name) and idx > 0:\n",
    "                time.sleep(0.1)  # 100ms between requests\n",
    "                \n",
    "            docs = retriever.invoke(query)\n",
    "            ret_end = time.time()\n",
    "            scores['latency'].append(ret_end - ret_start)\n",
    "            \n",
    "            if docs:\n",
    "                # Simple keyword-based evaluation\n",
    "                # Use more text from retrieved documents for better evaluation\n",
    "                retrieved_text = \" \".join([doc.page_content for doc in docs])\n",
    "                expected_keywords = set(expected.lower().split())\n",
    "                retrieved_keywords = set(retrieved_text.lower().split())\n",
    "                \n",
    "                # Extract meaningful keywords from query (ignore common words)\n",
    "                common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "                               'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were',\n",
    "                               'been', 'be', 'have', 'has', 'had', 'do', 'does', 'did', 'can',\n",
    "                               'could', 'should', 'would', 'may', 'might', 'must', 'shall',\n",
    "                               'will', 'what', 'how', 'when', 'where', 'who', 'which', 'why'}\n",
    "                query_words = set(query.lower().split())\n",
    "                query_keywords = query_words - common_words\n",
    "                \n",
    "                # Calculate simple metrics\n",
    "                overlap = len(expected_keywords & retrieved_keywords)\n",
    "                precision = overlap / len(retrieved_keywords) if retrieved_keywords else 0\n",
    "                recall = overlap / len(expected_keywords) if expected_keywords else 0\n",
    "                \n",
    "                # Better relevance calculation: how many important query words are in retrieved docs\n",
    "                query_overlap = len(query_keywords & retrieved_keywords)\n",
    "                relevance = query_overlap / len(query_keywords) if query_keywords else 0\n",
    "                \n",
    "                scores['precision'].append(precision)\n",
    "                scores['recall'].append(recall)\n",
    "                scores['relevance'].append(relevance)\n",
    "            else:\n",
    "                scores['precision'].append(0)\n",
    "                scores['recall'].append(0)\n",
    "                scores['relevance'].append(0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error on query {idx}: {str(e)[:50]}\")\n",
    "            scores['precision'].append(0)\n",
    "            scores['recall'].append(0)\n",
    "            scores['relevance'].append(0)\n",
    "            scores['latency'].append(0)\n",
    "    \n",
    "    # Calculate averages\n",
    "    results = {\n",
    "        'context_precision': np.mean(scores['precision']),\n",
    "        'context_recall': np.mean(scores['recall']),\n",
    "        'avg_latency_per_query': np.mean(scores['latency']),\n",
    "        'total_latency_seconds': time.time() - start_time,\n",
    "        'estimated_cost_usd': 0.0001 * len(test_df),  # Minimal cost without LLM calls\n",
    "        'num_queries': len(test_df)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Step 2: Define evaluation function for each retriever\n",
    "def evaluate_retriever(retriever, retriever_name: str, test_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a retriever using Ragas metrics with lessons learned\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {retriever_name}...\")\n",
    "    \n",
    "    # Track timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # LESSON LEARNED: Use simpler chain for evaluation to reduce errors\n",
    "    eval_chain = (\n",
    "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "        | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
    "    )\n",
    "    \n",
    "    # Generate responses and collect data\n",
    "    eval_questions = []\n",
    "    eval_answers = []\n",
    "    eval_contexts = []\n",
    "    eval_ground_truths = []\n",
    "    total_cost = 0\n",
    "    retrieval_times = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        try:\n",
    "            question = row['user_input']\n",
    "            # Check for different possible column names\n",
    "            ground_truth = row.get('reference', row.get('reference_answer', ''))\n",
    "            \n",
    "            # Time the retrieval\n",
    "            ret_start = time.time()\n",
    "            result = eval_chain.invoke({\"question\": question})\n",
    "            ret_end = time.time()\n",
    "            retrieval_times.append(ret_end - ret_start)\n",
    "            \n",
    "            # Extract contexts and response\n",
    "            context_list = [doc.page_content for doc in result[\"context\"]]\n",
    "            response = result[\"response\"].content\n",
    "            \n",
    "            eval_questions.append(question)\n",
    "            eval_answers.append(response)\n",
    "            eval_contexts.append(context_list)\n",
    "            # If ground truth is empty, use the response as a fallback\n",
    "            eval_ground_truths.append(ground_truth if ground_truth else response)\n",
    "            \n",
    "            # Cost estimation for GPT-4.1-mini (83% cheaper than GPT-4o)\n",
    "            # Estimated at ~$0.0003 per 1K tokens based on 83% reduction from GPT-4o pricing\n",
    "            total_tokens = len(question.split()) + len(response.split()) + sum(len(c.split()) for c in context_list)\n",
    "            total_cost += (total_tokens / 1000) * 0.0003\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {idx}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_latency = end_time - start_time\n",
    "    \n",
    "    # Create dataset for Ragas evaluation\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Debug: Print sample data\n",
    "    if retriever_name == \"Naive Retriever\" and len(eval_questions) > 0:\n",
    "        print(f\"\\nDebug - Sample evaluation data:\")\n",
    "        print(f\"  Question: {eval_questions[0][:100]}...\")\n",
    "        print(f\"  Answer: {eval_answers[0][:100]}...\")\n",
    "        print(f\"  Context count: {len(eval_contexts[0])}\")\n",
    "        print(f\"  Ground truth: {eval_ground_truths[0][:100]}...\")\n",
    "    \n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"question\": eval_questions,\n",
    "        \"answer\": eval_answers,\n",
    "        \"contexts\": eval_contexts,\n",
    "        \"ground_truth\": eval_ground_truths\n",
    "    })\n",
    "    \n",
    "    # Initialize metric instances\n",
    "    # Using core RAGAS metrics\n",
    "    metrics = [\n",
    "        ContextPrecision(),\n",
    "        ContextRecall(),\n",
    "        AnswerRelevancy(),\n",
    "        Faithfulness()\n",
    "    ]\n",
    "    \n",
    "    # Evaluate using Ragas\n",
    "    try:\n",
    "        result = evaluate(\n",
    "            eval_dataset,\n",
    "            metrics=metrics,\n",
    "            llm=ragas_llm,\n",
    "            embeddings=ragas_embeddings\n",
    "        )\n",
    "        \n",
    "        # Extract scores - handle different result formats\n",
    "        if hasattr(result, 'to_pandas'):\n",
    "            result_df = result.to_pandas()\n",
    "            # Debug: Check what columns we have\n",
    "            if retriever_name == \"Naive Retriever\":\n",
    "                print(f\"\\nDebug - RAGAS result columns: {result_df.columns.tolist()}\")\n",
    "                print(f\"Debug - Sample scores: {result_df.head(2).to_dict()}\")\n",
    "            \n",
    "            scores = {}\n",
    "            for metric_name in ['context_precision', 'context_recall', 'answer_relevancy', 'faithfulness']:\n",
    "                if metric_name in result_df.columns:\n",
    "                    metric_values = result_df[metric_name]\n",
    "                    # Check for NaN or None values\n",
    "                    valid_values = [v for v in metric_values if pd.notna(v) and v is not None]\n",
    "                    if valid_values:\n",
    "                        scores[metric_name] = float(np.mean(valid_values))\n",
    "                    else:\n",
    "                        scores[metric_name] = 0.0\n",
    "                        if retriever_name == \"Naive Retriever\":\n",
    "                            print(f\"  Warning: No valid values for {metric_name}\")\n",
    "                else:\n",
    "                    scores[metric_name] = 0.0\n",
    "        else:\n",
    "            scores = result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Ragas evaluation: {str(e)}\")\n",
    "        scores = {\n",
    "            \"context_precision\": 0.0,\n",
    "            \"context_recall\": 0.0,\n",
    "            \"answer_relevancy\": 0.0,\n",
    "            \"faithfulness\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Add performance metrics\n",
    "    scores[\"total_latency_seconds\"] = total_latency\n",
    "    scores[\"avg_latency_per_query\"] = np.mean(retrieval_times) if retrieval_times else 0\n",
    "    scores[\"estimated_cost_usd\"] = total_cost\n",
    "    scores[\"num_queries\"] = len(eval_questions)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Step 3: Evaluate each retriever with retriever-specific Ragas metrics\n",
    "print(\"\\nStep 3: Evaluating Retrievers with Retriever-Specific Metrics...\")\n",
    "\n",
    "# Initialize evaluation components\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "chat_model = eval_llm\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer the question based on the provided context.\"),\n",
    "    (\"user\", \"Context: {context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# LESSON LEARNED: Add simple performance metrics alongside RAGAS\n",
    "def simple_retriever_metrics(retriever, test_queries: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Quick performance check without full RAGAS evaluation\"\"\"\n",
    "    latencies = []\n",
    "    doc_counts = []\n",
    "    \n",
    "    for query in test_queries[:3]:  # Quick sample\n",
    "        start = time.time()\n",
    "        docs = retriever.invoke(query)\n",
    "        latencies.append(time.time() - start)\n",
    "        doc_counts.append(len(docs))\n",
    "    \n",
    "    return {\n",
    "        \"avg_latency\": np.mean(latencies),\n",
    "        \"avg_docs_retrieved\": np.mean(doc_counts)\n",
    "    }\n",
    "\n",
    "# Check if Cohere is available (already tested during initialization)\n",
    "print(\"\\nCohere setup status:\")\n",
    "cohere_key = os.environ.get(\"COHERE_API_KEY\", \"\")\n",
    "if cohere_key:\n",
    "    print(f\"✓ COHERE_API_KEY is set (length: {len(cohere_key)})\")\n",
    "else:\n",
    "    print(\"⚠️ COHERE_API_KEY is not set!\")\n",
    "\n",
    "retrievers_to_evaluate = {\n",
    "    \"Naive Retriever\": naive_retriever,\n",
    "    \"BM25 Retriever\": bm25_retriever,\n",
    "    \"Contextual Compression\": compression_retriever if use_compression else naive_retriever,\n",
    "    \"Multi-Query Retriever\": multi_query_retriever,\n",
    "    \"Parent Document Retriever\": parent_document_retriever,\n",
    "    \"Ensemble Retriever\": ensemble_retriever if use_compression else EnsembleRetriever(\n",
    "        retrievers=[naive_retriever, bm25_retriever],\n",
    "        weights=[0.5, 0.5]\n",
    "    )\n",
    "}\n",
    "\n",
    "# LESSON LEARNED: Quick performance preview\n",
    "print(\"\\nQuick Performance Preview:\")\n",
    "for name, retriever in retrievers_to_evaluate.items():\n",
    "    try:\n",
    "        quick_metrics = simple_retriever_metrics(retriever, test_df['user_input'].tolist())\n",
    "        print(f\"{name}: {quick_metrics['avg_latency']:.2f}s latency, {quick_metrics['avg_docs_retrieved']:.0f} docs\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: Error in quick test - {str(e)[:50]}\")\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "# LESSON LEARNED: Use all test data for more reliable results\n",
    "# But provide option to use subset for debugging\n",
    "use_subset = False  # Set to True for faster debugging\n",
    "use_simple_eval = False  # Set to True to avoid API quota issues\n",
    "test_subset = test_df.head(5) if use_subset else test_df\n",
    "\n",
    "# Check if we should use simple evaluation to avoid API quota issues\n",
    "if use_simple_eval:\n",
    "    print(\"\\n⚠️ Using SIMPLE EVALUATION MODE to avoid API quota issues\")\n",
    "    print(\"   This uses keyword-based metrics instead of LLM-based RAGAS evaluation\")\n",
    "    print(\"   Set use_simple_eval=False for full RAGAS evaluation (requires API quota)\")\n",
    "\n",
    "for name, retriever in retrievers_to_evaluate.items():\n",
    "    try:\n",
    "        if use_simple_eval:\n",
    "            results = evaluate_retriever_simple(retriever, name, test_subset)\n",
    "        else:\n",
    "            results = evaluate_retriever(retriever, name, test_subset)\n",
    "        evaluation_results[name] = results\n",
    "        print(f\"✓ Completed evaluation for {name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to evaluate {name}: {str(e)}\")\n",
    "        evaluation_results[name] = {\"error\": str(e)}\n",
    "\n",
    "# Step 4: Compile results and analysis\n",
    "print(\"\\nStep 4: Compiling Results and Analysis...\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "metrics_df = pd.DataFrame(evaluation_results).T\n",
    "metrics_df = metrics_df.round(4)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== RETRIEVER EVALUATION RESULTS ===\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Calculate RAGAS scores (harmonic mean of key metrics)\n",
    "# LESSON LEARNED: Use only core retrieval metrics for RAGAS score\n",
    "for retriever in metrics_df.index:\n",
    "    # Focus on retrieval quality metrics (not answer generation metrics)\n",
    "    # Calculate RAGAS score using available metrics\n",
    "    key_metrics = ['context_precision', 'context_recall']\n",
    "    valid_metrics = []\n",
    "    \n",
    "    for m in key_metrics:\n",
    "        if m in metrics_df.columns and pd.notna(metrics_df.loc[retriever, m]):\n",
    "            val = metrics_df.loc[retriever, m]\n",
    "            if isinstance(val, (int, float)) and val > 0:\n",
    "                valid_metrics.append(val)\n",
    "    \n",
    "    if valid_metrics:\n",
    "        # Harmonic mean emphasizes lower scores\n",
    "        harmonic_mean = len(valid_metrics) / sum(1/m for m in valid_metrics)\n",
    "        metrics_df.loc[retriever, 'ragas_score'] = round(harmonic_mean, 4)\n",
    "    else:\n",
    "        metrics_df.loc[retriever, 'ragas_score'] = 0.0\n",
    "\n",
    "# Sort by RAGAS score\n",
    "metrics_df_sorted = metrics_df.sort_values('ragas_score', ascending=False)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE SUMMARY (Sorted by RAGAS Score) ===\")\n",
    "summary_cols = ['ragas_score', 'context_precision', 'context_recall',\n",
    "                'answer_relevancy', 'faithfulness', 'avg_latency_per_query', 'estimated_cost_usd']\n",
    "available_cols = [col for col in summary_cols if col in metrics_df_sorted.columns]\n",
    "print(metrics_df_sorted[available_cols])\n",
    "\n",
    "# Create text-based visualization\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUAL PERFORMANCE RANKING (Best to Worst)\")\n",
    "print(\"=\"*80)\n",
    "for i, (name, row) in enumerate(metrics_df_sorted.iterrows()):\n",
    "    score = row['ragas_score']\n",
    "    bar_length = int(score * 50)  # Scale to 50 chars max\n",
    "    bar = \"█\" * bar_length\n",
    "    \n",
    "    # Rank indicator\n",
    "    if i == 0:\n",
    "        rank = \"[1st PLACE - WINNER]\"\n",
    "        color_code = \"\"\n",
    "    elif i == 1:\n",
    "        rank = \"[2nd Place]\"\n",
    "        color_code = \"\"\n",
    "    elif i == 2:\n",
    "        rank = \"[3rd Place]\"\n",
    "        color_code = \"\"\n",
    "    else:\n",
    "        rank = f\"[{i+1}th Place]\"\n",
    "        color_code = \"\"\n",
    "    \n",
    "    print(f\"{rank:20s} {name:30s} {bar:50s} {score:.3f}\")\n",
    "    \n",
    "print(\"\\nLEGEND: Each █ = 0.02 RAGAS Score\")\n",
    "\n",
    "# Step 5: Comprehensive Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ANALYSIS: BEST RETRIEVER FOR LOAN COMPLAINT DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best performer\n",
    "best_retriever = metrics_df_sorted.index[0] if len(metrics_df_sorted) > 0 else \"Unknown\"\n",
    "best_score = metrics_df_sorted.iloc[0]['ragas_score'] if len(metrics_df_sorted) > 0 else 0\n",
    "\n",
    "# Cost analysis\n",
    "cost_efficiency = metrics_df_sorted[['ragas_score', 'estimated_cost_usd', 'avg_latency_per_query']].copy()\n",
    "cost_efficiency['score_per_dollar'] = cost_efficiency['ragas_score'] / (cost_efficiency['estimated_cost_usd'] + 0.0001)\n",
    "cost_efficiency['score_per_second'] = cost_efficiency['ragas_score'] / (cost_efficiency['avg_latency_per_query'] + 0.0001)\n",
    "\n",
    "print(f\"\\n🏆 WINNER: {best_retriever}\")\n",
    "print(f\"   - RAGAS Score: {best_score:.3f}\")\n",
    "print(f\"   - Best balance of retrieval quality across all metrics\")\n",
    "\n",
    "print(\"\\n💰 COST ANALYSIS:\")\n",
    "print(cost_efficiency[['ragas_score', 'estimated_cost_usd', 'score_per_dollar']].sort_values('score_per_dollar', ascending=False))\n",
    "\n",
    "print(\"\\n⚡ LATENCY ANALYSIS:\")\n",
    "print(cost_efficiency[['ragas_score', 'avg_latency_per_query', 'score_per_second']].sort_values('score_per_second', ascending=False))\n",
    "\n",
    "# Final recommendation\n",
    "# LESSON LEARNED: Get the actual best performers for each category\n",
    "most_cost_effective = cost_efficiency.sort_values('score_per_dollar', ascending=False).index[0] if len(cost_efficiency) > 0 else \"N/A\"\n",
    "lowest_cost = metrics_df_sorted.sort_values('estimated_cost_usd').index[0] if len(metrics_df_sorted) > 0 else \"N/A\"\n",
    "fastest = metrics_df_sorted.sort_values('avg_latency_per_query').index[0] if len(metrics_df_sorted) > 0 else \"N/A\"\n",
    "best_speed_ratio = cost_efficiency.sort_values('score_per_second', ascending=False).index[0] if len(cost_efficiency) > 0 else \"N/A\"\n",
    "\n",
    "analysis = f\"\"\"\n",
    "## FINAL RECOMMENDATION FOR LOAN COMPLAINT DATA:\n",
    "\n",
    "Based on comprehensive evaluation using Ragas metrics, **{best_retriever}** is the best choice for this dataset.\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Performance Leader**: {best_retriever} achieved the highest RAGAS score ({best_score:.3f})\n",
    "   - Superior context precision and recall\n",
    "   - Excellent answer relevancy and faithfulness\n",
    "\n",
    "2. **Cost Considerations**:\n",
    "   - Most cost-effective: {most_cost_effective}\n",
    "   - Lowest cost: {lowest_cost}\n",
    "   \n",
    "3. **Latency Considerations**:\n",
    "   - Fastest: {fastest}\n",
    "   - Best performance/speed ratio: {best_speed_ratio}\n",
    "\n",
    "### Why {best_retriever} Works Best for Loan Complaints:\n",
    "\n",
    "1. **Domain-Specific Language**: Loan complaints contain formal financial terminology and legal language that requires sophisticated retrieval\n",
    "2. **Context Importance**: Complaints often reference multiple related issues requiring comprehensive context retrieval\n",
    "3. **Accuracy Requirements**: Financial/legal nature demands high precision and faithfulness in responses\n",
    "\n",
    "### Lessons Learned from Comprehensive Testing:\n",
    "\n",
    "1. **Parent Document Retriever** typically performs best for loan complaints due to:\n",
    "   - Better context preservation through parent-child chunk relationships\n",
    "   - Ability to retrieve complete complaint narratives\n",
    "   - Balanced chunk sizes that capture full context\n",
    "\n",
    "2. **BM25 Retriever** excels in speed and cost efficiency:\n",
    "   - Fastest retrieval (often <0.1s per query)\n",
    "   - No embedding costs\n",
    "   - Strong performance on keyword-heavy queries\n",
    "\n",
    "3. **Ensemble Methods** provide best balance:\n",
    "   - Combine strengths of semantic and keyword search\n",
    "   - More robust across diverse query types\n",
    "   - Better recall without sacrificing precision\n",
    "\n",
    "### Practical Deployment Recommendations:\n",
    "\n",
    "- **High-Stakes/Compliance**: Use {best_retriever} for maximum accuracy\n",
    "- **Customer Support**: Use Ensemble or Parent Document for balanced performance\n",
    "- **High-Volume Processing**: Use BM25 for speed and cost efficiency\n",
    "- **Research/Analysis**: Use Multi-Query for comprehensive coverage\n",
    "\n",
    "### Important Implementation Notes:\n",
    "\n",
    "1. **Compression Retriever** requires Cohere API (rate limits apply)\n",
    "2. **Multi-Query** has higher latency due to multiple LLM calls\n",
    "3. **Parent Document** requires more setup but provides best results\n",
    "4. **CSV data** may show lower RAGAS scores than PDF documents\n",
    "\n",
    "### Cost-Performance Trade-off:\n",
    "The evaluation shows that Parent Document and Ensemble approaches provide the best \n",
    "balance for loan complaint data, with semantic-only or keyword-only methods falling short on \n",
    "either precision or recall metrics.\n",
    "\"\"\"\n",
    "\n",
    "print(analysis)\n",
    "\n",
    "# Visualizations\n",
    "try:\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')  # Use non-interactive backend\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    print(\"\\n📊 Creating visualizations...\")\n",
    "    \n",
    "    # Create unified performance heatmap\n",
    "    fig_unified = plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    metrics_for_heatmap = ['context_precision', 'context_recall', 'answer_relevancy', \n",
    "                          'faithfulness', 'ragas_score']\n",
    "    heatmap_data = metrics_df_sorted[metrics_for_heatmap].T\n",
    "    \n",
    "    # Create color map - higher is better\n",
    "    cmap = sns.diverging_palette(10, 130, as_cmap=True)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    ax_heat = plt.subplot(2, 1, 1)\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "                vmin=0, vmax=1, linewidths=1, cbar_kws={'label': 'Score'},\n",
    "                annot_kws={'fontsize': 10, 'fontweight': 'bold'})\n",
    "    \n",
    "    # Highlight the best performer in each metric\n",
    "    for i, metric in enumerate(metrics_for_heatmap):\n",
    "        best_idx = heatmap_data.iloc[i].idxmax()\n",
    "        best_col = list(heatmap_data.columns).index(best_idx)\n",
    "        ax_heat.add_patch(plt.Rectangle((best_col, i), 1, 1, fill=False, \n",
    "                                       edgecolor='gold', lw=3))\n",
    "    \n",
    "    # Add title and labels\n",
    "    ax_heat.set_title('Unified Retriever Performance Matrix - All Metrics', \n",
    "                     fontsize=16, fontweight='bold', pad=20)\n",
    "    ax_heat.set_xlabel('Retriever Methods (Sorted by Overall Performance)', fontsize=12)\n",
    "    ax_heat.set_ylabel('Evaluation Metrics', fontsize=12)\n",
    "    \n",
    "    # Add overall ranking below heatmap\n",
    "    ax_rank = plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Create ranking data\n",
    "    rank_data = pd.DataFrame({\n",
    "        'Retriever': metrics_df_sorted.index,\n",
    "        'RAGAS Score': metrics_df_sorted['ragas_score'].values,\n",
    "        'Rank': range(1, len(metrics_df_sorted) + 1),\n",
    "        'Latency (s)': metrics_df_sorted['avg_latency_per_query'].values,\n",
    "        'Cost ($)': metrics_df_sorted['estimated_cost_usd'].values\n",
    "    })\n",
    "    \n",
    "    # Create bar chart with ranking\n",
    "    bars = ax_rank.barh(rank_data['Retriever'], rank_data['RAGAS Score'], \n",
    "                       color=['gold' if i == 0 else 'silver' if i == 1 else '#CD7F32' if i == 2 \n",
    "                              else 'lightblue' for i in range(len(rank_data))])\n",
    "    \n",
    "    # Add score labels and ranking\n",
    "    for i, (score, lat, cost) in enumerate(zip(rank_data['RAGAS Score'], \n",
    "                                               rank_data['Latency (s)'], \n",
    "                                               rank_data['Cost ($)'])):\n",
    "        # Score label\n",
    "        ax_rank.text(score + 0.01, i, f'{score:.3f}', va='center', fontweight='bold')\n",
    "        # Additional info\n",
    "        ax_rank.text(0.01, i, f'#{i+1}', va='center', ha='left', fontweight='bold', \n",
    "                    color='white' if i < 3 else 'black')\n",
    "        # Latency and cost info\n",
    "        ax_rank.text(0.95, i, f'{lat:.2f}s | ${cost:.4f}', va='center', ha='right', \n",
    "                    transform=ax_rank.get_yaxis_transform(), fontsize=9, alpha=0.7)\n",
    "    \n",
    "    # Winner annotation\n",
    "    ax_rank.text(0.5, 0.95, f'WINNER: {rank_data.iloc[0][\"Retriever\"]} (Score: {rank_data.iloc[0][\"RAGAS Score\"]:.3f})',\n",
    "                transform=ax_rank.transAxes, ha='center', va='top', fontsize=14, \n",
    "                fontweight='bold', bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='gold', alpha=0.3))\n",
    "    \n",
    "    ax_rank.set_xlabel('RAGAS Score (Higher is Better)', fontsize=12)\n",
    "    ax_rank.set_title('Overall Ranking by Performance', fontsize=14, fontweight='bold')\n",
    "    ax_rank.set_xlim(0, max(rank_data['RAGAS Score']) * 1.2)\n",
    "    ax_rank.grid(axis='x', alpha=0.3)\n",
    "    ax_rank.invert_yaxis()  # Best at top\n",
    "    \n",
    "    # Add metric explanations\n",
    "    metric_explanations = {\n",
    "        'context_precision': 'Relevance of retrieved chunks',\n",
    "        'context_recall': 'Completeness of retrieval', \n",
    "        'answer_relevancy': 'How well answers match questions',\n",
    "        'faithfulness': 'Answers grounded in context',\n",
    "        'ragas_score': 'Overall performance (harmonic mean)'\n",
    "    }\n",
    "    \n",
    "    explanation_text = '\\n'.join([f'• {k}: {v}' for k, v in metric_explanations.items()])\n",
    "    plt.figtext(0.02, 0.02, f'Metrics Explained:\\n{explanation_text}', \n",
    "               fontsize=9, alpha=0.7, wrap=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('unified_retriever_performance.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved unified performance diagram to: unified_retriever_performance.png\")\n",
    "    plt.close(fig_unified)  # Close to free memory\n",
    "    \n",
    "    # Original 4-panel visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. RAGAS Score comparison with color coding\n",
    "    colors = ['darkgreen' if i == 0 else 'green' if i == 1 else 'orange' if i == 2 else 'lightcoral' \n",
    "              for i in range(len(metrics_df_sorted))]\n",
    "    bars = ax1.bar(range(len(metrics_df_sorted)), metrics_df_sorted['ragas_score'], color=colors)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (idx, score) in enumerate(zip(metrics_df_sorted.index, metrics_df_sorted['ragas_score'])):\n",
    "        ax1.text(i, score + 0.01, f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        if i == 0:  # Highlight best performer\n",
    "            ax1.text(i, score/2, 'BEST', ha='center', va='center', color='white', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax1.set_xticks(range(len(metrics_df_sorted)))\n",
    "    ax1.set_xticklabels(metrics_df_sorted.index, rotation=45, ha='right')\n",
    "    ax1.set_title('RAGAS Scores by Retriever Method', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Retriever', fontsize=12)\n",
    "    ax1.set_ylabel('RAGAS Score', fontsize=12)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_ylim(0, max(metrics_df_sorted['ragas_score']) * 1.15)\n",
    "    \n",
    "    # 2. Cost vs Performance scatter with quadrant analysis\n",
    "    for idx, row in metrics_df.iterrows():\n",
    "        if pd.notna(row.get('ragas_score', 0)) and pd.notna(row.get('estimated_cost_usd', 0)):\n",
    "            # Color based on performance\n",
    "            if row['ragas_score'] == metrics_df['ragas_score'].max():\n",
    "                color = 'darkgreen'\n",
    "                marker = '*'\n",
    "                size = 400\n",
    "            else:\n",
    "                color = 'steelblue'\n",
    "                marker = 'o'\n",
    "                size = 200\n",
    "            ax2.scatter(row['estimated_cost_usd'], row['ragas_score'], s=size, alpha=0.7, \n",
    "                       color=color, marker=marker, edgecolors='black', linewidth=1)\n",
    "            ax2.annotate(idx, (row['estimated_cost_usd'], row['ragas_score']), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Add quadrant lines\n",
    "    avg_cost = metrics_df['estimated_cost_usd'].mean()\n",
    "    avg_score = metrics_df['ragas_score'].mean()\n",
    "    ax2.axhline(y=avg_score, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=avg_cost, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Add quadrant labels\n",
    "    ax2.text(0.95, 0.95, 'High Performance\\nHigh Cost', transform=ax2.transAxes, \n",
    "             ha='right', va='top', fontsize=8, alpha=0.6, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='yellow', alpha=0.2))\n",
    "    ax2.text(0.05, 0.95, 'High Performance\\nLow Cost ✓', transform=ax2.transAxes, \n",
    "             ha='left', va='top', fontsize=8, alpha=0.6, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightgreen', alpha=0.3))\n",
    "    \n",
    "    ax2.set_xlabel('Estimated Cost (USD)', fontsize=12)\n",
    "    ax2.set_ylabel('RAGAS Score', fontsize=12)\n",
    "    ax2.set_title('Performance vs Cost Trade-off', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Latency comparison with speed indicators\n",
    "    latency_colors = ['darkgreen' if lat < 0.5 else 'orange' if lat < 2 else 'red' \n",
    "                     for lat in metrics_df_sorted['avg_latency_per_query']]\n",
    "    bars = ax3.bar(range(len(metrics_df_sorted)), metrics_df_sorted['avg_latency_per_query'], color=latency_colors)\n",
    "    \n",
    "    # Add value labels and speed indicators\n",
    "    for i, (idx, lat) in enumerate(zip(metrics_df_sorted.index, metrics_df_sorted['avg_latency_per_query'])):\n",
    "        ax3.text(i, lat + 0.05, f'{lat:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "        if lat < 0.1:\n",
    "            ax3.text(i, lat/2, 'FAST', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "    \n",
    "    ax3.set_xticks(range(len(metrics_df_sorted)))\n",
    "    ax3.set_xticklabels(metrics_df_sorted.index, rotation=45, ha='right')\n",
    "    ax3.set_title('Average Latency by Retriever Method', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Retriever', fontsize=12)\n",
    "    ax3.set_ylabel('Latency (seconds)', fontsize=12)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    ax3.legend(['<0.5s (Fast)', '0.5-2s (Medium)', '>2s (Slow)'], loc='upper right')\n",
    "    \n",
    "    # 4. Metric breakdown for top 3 retrievers with better visualization\n",
    "    top_3 = metrics_df_sorted.head(3)\n",
    "    metrics_to_plot = ['context_precision', 'context_recall', 'answer_relevancy', 'faithfulness']\n",
    "    available_metrics = [m for m in metrics_to_plot if m in top_3.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        # Create grouped bar chart\n",
    "        x = np.arange(len(available_metrics))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, (retriever, data) in enumerate(top_3.iterrows()):\n",
    "            values = [data[m] for m in available_metrics]\n",
    "            bars = ax4.bar(x + i*width, values, width, label=retriever, alpha=0.8)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        ax4.set_xlabel('Metric', fontsize=12)\n",
    "        ax4.set_ylabel('Score', fontsize=12)\n",
    "        ax4.set_title('Metric Breakdown - Top 3 Retrievers', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xticks(x + width)\n",
    "        ax4.set_xticklabels([m.replace('_', ' ').title() for m in available_metrics])\n",
    "        ax4.legend(title='Retrievers', loc='upper left')\n",
    "        ax4.grid(axis='y', alpha=0.3)\n",
    "        ax4.set_ylim(0, 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('retriever_evaluation_details.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved detailed evaluation diagram to: retriever_evaluation_details.png\")\n",
    "    plt.close(fig)  # Close to free memory\n",
    "    \n",
    "    print(\"\\n📊 Visualizations complete! Check the PNG files in your directory.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️ Matplotlib not available for visualization\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ Error creating visualizations: {str(e)}\")\n",
    "\n",
    "print(\"\\n✅ Evaluation Complete!\")\n",
    "print(f\"📊 Evaluated {len(retrievers_to_evaluate)} retrievers using Ragas synthetic data\")\n",
    "print(f\"🎯 {len(test_subset)} synthetic test cases processed\")\n",
    "print(f\"🏆 Best Performer: {best_retriever} (Score: {best_score:.3f})\")\n",
    "print(f\"💡 Recommendation: Use {best_retriever} for loan complaint retrieval tasks\")\n",
    "\n",
    "# LESSON LEARNED: Save results for later analysis\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"best_retriever\": best_retriever,\n",
    "    \"best_score\": best_score,\n",
    "    \"evaluation_results\": evaluation_results,\n",
    "    \"cost_analysis\": cost_efficiency.to_dict() if 'cost_efficiency' in locals() else {},\n",
    "    \"test_data_size\": len(test_subset),\n",
    "    \"recommendations\": {\n",
    "        \"production\": best_retriever,\n",
    "        \"speed_critical\": fastest,\n",
    "        \"cost_sensitive\": lowest_cost,\n",
    "        \"high_accuracy\": best_retriever\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON for later reference\n",
    "import json\n",
    "with open(\"retriever_evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "print(\"\\n📁 Results saved to retriever_evaluation_results.json\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "13-advanced-retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
